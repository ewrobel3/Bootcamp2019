{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow & Keras - Basics of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most importantly... resources\n",
    "\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF overview\n",
    "\n",
    "* #### \"End-to-end machine learning platform\" \n",
    "\n",
    "    - Not the only one! Check out PyTorch, Theano, Cognitive Toolkit.\n",
    "    - Used to just be deep learning - they've expanded recently\n",
    "   \n",
    "* #### Integrates with high-level APIs like Keras\n",
    "* #### Plays nice with Pandas\n",
    "* #### Makes deep learning *fast* and *easy* *\n",
    "    *<sup>\"easy\"</sup>\n",
    "\n",
    "## Tasks for TensorFlow:\n",
    "\n",
    "* #### Regression\n",
    "    - Predict house prices\n",
    "    - Predict drug metabolic rates\n",
    "    - Predict stock trends *\n",
    "    \n",
    "    *<sup>this is super hard</sup>\n",
    "    \n",
    "    \n",
    "\n",
    "* #### Classification\n",
    "    - Cat or dog?\n",
    "    - Malignant or benign cancer from images\n",
    "    ![](media/dr.png)\n",
    "    <span style=\"font-size:0.75em;\">Google AI Blog: Diabetic Retinopathy</span>\n",
    "\n",
    "\n",
    "\n",
    "* #### Dimensionality reduction\n",
    "    - Visualize high-dimensional data in 2 or 3-D space\n",
    "    - Compress representations for successive ML\n",
    "\n",
    "\n",
    "\n",
    "* #### Generative models\n",
    "    - Create new molecules with desirable properties\n",
    "    - Artificially enhance image resolution\n",
    "    ![](media/molecular_gan.png)\n",
    "    <span style=\"font-size:0.75em;\">Kadurin et al., 2017</span>\n",
    "\n",
    "\n",
    "* #### Reinforcement learning\n",
    "    - Can't beat your friends at chess? Make your computer do it\n",
    "\n",
    "\n",
    "\n",
    "* #### Much more...\n",
    "    - Generic math\n",
    "    - Probabilistic programming with TFP\n",
    "    - Automatic differentiation\n",
    "    - ...\n",
    "\n",
    "\n",
    "## Let's Regress\n",
    "\n",
    "### Imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name a more iconic duo, I'll wait\n",
    "\n",
    "#### New imports -- TF and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check our versions for good measure -- these programs may have very different behavior version-to-version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading in housing data as with SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1736800520</td>\n",
       "      <td>20150403T000000</td>\n",
       "      <td>662500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3560</td>\n",
       "      <td>9796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1860</td>\n",
       "      <td>1700</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98007</td>\n",
       "      <td>47.6007</td>\n",
       "      <td>-122.145</td>\n",
       "      <td>2210</td>\n",
       "      <td>8925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9212900260</td>\n",
       "      <td>20140527T000000</td>\n",
       "      <td>468000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1160</td>\n",
       "      <td>6000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>300</td>\n",
       "      <td>1942</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6900</td>\n",
       "      <td>-122.292</td>\n",
       "      <td>1330</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>114101516</td>\n",
       "      <td>20140528T000000</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1430</td>\n",
       "      <td>19901</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>1927</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7558</td>\n",
       "      <td>-122.229</td>\n",
       "      <td>1780</td>\n",
       "      <td>12697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6054650070</td>\n",
       "      <td>20141007T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1370</td>\n",
       "      <td>9680</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1370</td>\n",
       "      <td>0</td>\n",
       "      <td>1977</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6127</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1370</td>\n",
       "      <td>10208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1175000570</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>530000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1810</td>\n",
       "      <td>4850</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1810</td>\n",
       "      <td>0</td>\n",
       "      <td>1900</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6700</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1360</td>\n",
       "      <td>4850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9297300055</td>\n",
       "      <td>20150124T000000</td>\n",
       "      <td>650000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>2950</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1980</td>\n",
       "      <td>970</td>\n",
       "      <td>1979</td>\n",
       "      <td>0</td>\n",
       "      <td>98126</td>\n",
       "      <td>47.5714</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>2140</td>\n",
       "      <td>4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1875500060</td>\n",
       "      <td>20140731T000000</td>\n",
       "      <td>395000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1890</td>\n",
       "      <td>14040</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>1994</td>\n",
       "      <td>0</td>\n",
       "      <td>98019</td>\n",
       "      <td>47.7277</td>\n",
       "      <td>-121.962</td>\n",
       "      <td>1890</td>\n",
       "      <td>14018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6865200140</td>\n",
       "      <td>20140529T000000</td>\n",
       "      <td>485000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1600</td>\n",
       "      <td>4300</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1916</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6648</td>\n",
       "      <td>-122.343</td>\n",
       "      <td>1610</td>\n",
       "      <td>4300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>16000397</td>\n",
       "      <td>20141205T000000</td>\n",
       "      <td>189000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1200</td>\n",
       "      <td>9850</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>0</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3089</td>\n",
       "      <td>-122.210</td>\n",
       "      <td>1060</td>\n",
       "      <td>5095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7983200060</td>\n",
       "      <td>20150424T000000</td>\n",
       "      <td>230000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1250</td>\n",
       "      <td>9774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1250</td>\n",
       "      <td>0</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3343</td>\n",
       "      <td>-122.306</td>\n",
       "      <td>1280</td>\n",
       "      <td>8850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6300500875</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1620</td>\n",
       "      <td>4980</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>860</td>\n",
       "      <td>760</td>\n",
       "      <td>1947</td>\n",
       "      <td>0</td>\n",
       "      <td>98133</td>\n",
       "      <td>47.7025</td>\n",
       "      <td>-122.341</td>\n",
       "      <td>1400</td>\n",
       "      <td>4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2524049179</td>\n",
       "      <td>20140826T000000</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3050</td>\n",
       "      <td>44867</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2330</td>\n",
       "      <td>720</td>\n",
       "      <td>1968</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5316</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>4110</td>\n",
       "      <td>20336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7137970340</td>\n",
       "      <td>20140703T000000</td>\n",
       "      <td>285000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>6300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98092</td>\n",
       "      <td>47.3266</td>\n",
       "      <td>-122.169</td>\n",
       "      <td>2240</td>\n",
       "      <td>7005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8091400200</td>\n",
       "      <td>20140516T000000</td>\n",
       "      <td>252700.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1070</td>\n",
       "      <td>9643</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1070</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3533</td>\n",
       "      <td>-122.166</td>\n",
       "      <td>1220</td>\n",
       "      <td>8386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3814700200</td>\n",
       "      <td>20141120T000000</td>\n",
       "      <td>329000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2450</td>\n",
       "      <td>6500</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2450</td>\n",
       "      <td>0</td>\n",
       "      <td>1985</td>\n",
       "      <td>0</td>\n",
       "      <td>98030</td>\n",
       "      <td>47.3739</td>\n",
       "      <td>-122.172</td>\n",
       "      <td>2200</td>\n",
       "      <td>6865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1202000200</td>\n",
       "      <td>20141103T000000</td>\n",
       "      <td>233000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1710</td>\n",
       "      <td>4697</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1710</td>\n",
       "      <td>0</td>\n",
       "      <td>1941</td>\n",
       "      <td>0</td>\n",
       "      <td>98002</td>\n",
       "      <td>47.3048</td>\n",
       "      <td>-122.218</td>\n",
       "      <td>1030</td>\n",
       "      <td>4705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1794500383</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>937000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>2450</td>\n",
       "      <td>2691</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1750</td>\n",
       "      <td>700</td>\n",
       "      <td>1915</td>\n",
       "      <td>0</td>\n",
       "      <td>98119</td>\n",
       "      <td>47.6386</td>\n",
       "      <td>-122.360</td>\n",
       "      <td>1760</td>\n",
       "      <td>3573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3303700376</td>\n",
       "      <td>20141201T000000</td>\n",
       "      <td>667000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1581</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1400</td>\n",
       "      <td>0</td>\n",
       "      <td>1909</td>\n",
       "      <td>0</td>\n",
       "      <td>98112</td>\n",
       "      <td>47.6221</td>\n",
       "      <td>-122.314</td>\n",
       "      <td>1860</td>\n",
       "      <td>3861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5101402488</td>\n",
       "      <td>20140624T000000</td>\n",
       "      <td>438000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1520</td>\n",
       "      <td>6380</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>790</td>\n",
       "      <td>730</td>\n",
       "      <td>1948</td>\n",
       "      <td>0</td>\n",
       "      <td>98115</td>\n",
       "      <td>47.6950</td>\n",
       "      <td>-122.304</td>\n",
       "      <td>1520</td>\n",
       "      <td>6235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1873100390</td>\n",
       "      <td>20150302T000000</td>\n",
       "      <td>719000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2570</td>\n",
       "      <td>7173</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98052</td>\n",
       "      <td>47.7073</td>\n",
       "      <td>-122.110</td>\n",
       "      <td>2630</td>\n",
       "      <td>6026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21583</th>\n",
       "      <td>2025049203</td>\n",
       "      <td>20140610T000000</td>\n",
       "      <td>399950.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>710</td>\n",
       "      <td>1157</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>710</td>\n",
       "      <td>0</td>\n",
       "      <td>1943</td>\n",
       "      <td>0</td>\n",
       "      <td>98102</td>\n",
       "      <td>47.6413</td>\n",
       "      <td>-122.329</td>\n",
       "      <td>1370</td>\n",
       "      <td>1173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21584</th>\n",
       "      <td>952006823</td>\n",
       "      <td>20141202T000000</td>\n",
       "      <td>380000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1260</td>\n",
       "      <td>900</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>940</td>\n",
       "      <td>320</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5621</td>\n",
       "      <td>-122.384</td>\n",
       "      <td>1310</td>\n",
       "      <td>1415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21585</th>\n",
       "      <td>3832050760</td>\n",
       "      <td>20140828T000000</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1870</td>\n",
       "      <td>5000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1870</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3339</td>\n",
       "      <td>-122.055</td>\n",
       "      <td>2170</td>\n",
       "      <td>5399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21586</th>\n",
       "      <td>2767604724</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>505000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1430</td>\n",
       "      <td>1201</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1430</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98107</td>\n",
       "      <td>47.6707</td>\n",
       "      <td>-122.381</td>\n",
       "      <td>1430</td>\n",
       "      <td>1249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21587</th>\n",
       "      <td>6632300207</td>\n",
       "      <td>20150305T000000</td>\n",
       "      <td>385000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1520</td>\n",
       "      <td>1488</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1520</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7337</td>\n",
       "      <td>-122.309</td>\n",
       "      <td>1520</td>\n",
       "      <td>1497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>2767600688</td>\n",
       "      <td>20141113T000000</td>\n",
       "      <td>414500.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1210</td>\n",
       "      <td>1278</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1020</td>\n",
       "      <td>190</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98117</td>\n",
       "      <td>47.6756</td>\n",
       "      <td>-122.375</td>\n",
       "      <td>1210</td>\n",
       "      <td>1118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21589</th>\n",
       "      <td>7570050450</td>\n",
       "      <td>20140910T000000</td>\n",
       "      <td>347500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2540</td>\n",
       "      <td>4760</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2540</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3452</td>\n",
       "      <td>-122.022</td>\n",
       "      <td>2540</td>\n",
       "      <td>4571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21590</th>\n",
       "      <td>7430200100</td>\n",
       "      <td>20140514T000000</td>\n",
       "      <td>1222500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>4910</td>\n",
       "      <td>9444</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>3110</td>\n",
       "      <td>1800</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6502</td>\n",
       "      <td>-122.066</td>\n",
       "      <td>4560</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21591</th>\n",
       "      <td>4140940150</td>\n",
       "      <td>20141002T000000</td>\n",
       "      <td>572000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2770</td>\n",
       "      <td>3852</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2770</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5001</td>\n",
       "      <td>-122.232</td>\n",
       "      <td>1810</td>\n",
       "      <td>5641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21592</th>\n",
       "      <td>1931300412</td>\n",
       "      <td>20150416T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1190</td>\n",
       "      <td>1200</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1190</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6542</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1180</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21593</th>\n",
       "      <td>8672200110</td>\n",
       "      <td>20150317T000000</td>\n",
       "      <td>1088000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4170</td>\n",
       "      <td>8142</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>4170</td>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5354</td>\n",
       "      <td>-122.181</td>\n",
       "      <td>3030</td>\n",
       "      <td>7980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>5087900040</td>\n",
       "      <td>20141017T000000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.75</td>\n",
       "      <td>2500</td>\n",
       "      <td>5995</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2500</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98042</td>\n",
       "      <td>47.3749</td>\n",
       "      <td>-122.107</td>\n",
       "      <td>2530</td>\n",
       "      <td>5988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>1972201967</td>\n",
       "      <td>20141031T000000</td>\n",
       "      <td>520000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1530</td>\n",
       "      <td>981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1480</td>\n",
       "      <td>50</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6533</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>7502800100</td>\n",
       "      <td>20140813T000000</td>\n",
       "      <td>679950.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2.75</td>\n",
       "      <td>3600</td>\n",
       "      <td>9437</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3600</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98059</td>\n",
       "      <td>47.4822</td>\n",
       "      <td>-122.131</td>\n",
       "      <td>3550</td>\n",
       "      <td>9421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>191100405</td>\n",
       "      <td>20150421T000000</td>\n",
       "      <td>1575000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.25</td>\n",
       "      <td>3410</td>\n",
       "      <td>10125</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>3410</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>0</td>\n",
       "      <td>98040</td>\n",
       "      <td>47.5653</td>\n",
       "      <td>-122.223</td>\n",
       "      <td>2290</td>\n",
       "      <td>10125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>8956200760</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>541800.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3118</td>\n",
       "      <td>7866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3118</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98001</td>\n",
       "      <td>47.2931</td>\n",
       "      <td>-122.264</td>\n",
       "      <td>2673</td>\n",
       "      <td>6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>7202300110</td>\n",
       "      <td>20140915T000000</td>\n",
       "      <td>810000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3990</td>\n",
       "      <td>7838</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>3990</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6857</td>\n",
       "      <td>-122.046</td>\n",
       "      <td>3370</td>\n",
       "      <td>6814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21600</th>\n",
       "      <td>249000205</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>1537000.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.75</td>\n",
       "      <td>4470</td>\n",
       "      <td>8088</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>4470</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98004</td>\n",
       "      <td>47.6321</td>\n",
       "      <td>-122.200</td>\n",
       "      <td>2780</td>\n",
       "      <td>8964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21601</th>\n",
       "      <td>5100403806</td>\n",
       "      <td>20150407T000000</td>\n",
       "      <td>467000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1425</td>\n",
       "      <td>1179</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1425</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.6963</td>\n",
       "      <td>-122.318</td>\n",
       "      <td>1285</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21602</th>\n",
       "      <td>844000965</td>\n",
       "      <td>20140626T000000</td>\n",
       "      <td>224000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>11968</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98010</td>\n",
       "      <td>47.3095</td>\n",
       "      <td>-122.002</td>\n",
       "      <td>1320</td>\n",
       "      <td>11303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21603</th>\n",
       "      <td>7852140040</td>\n",
       "      <td>20140825T000000</td>\n",
       "      <td>507250.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2270</td>\n",
       "      <td>5536</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2270</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98065</td>\n",
       "      <td>47.5389</td>\n",
       "      <td>-121.881</td>\n",
       "      <td>2270</td>\n",
       "      <td>5731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21604</th>\n",
       "      <td>9834201367</td>\n",
       "      <td>20150126T000000</td>\n",
       "      <td>429000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1490</td>\n",
       "      <td>1126</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5699</td>\n",
       "      <td>-122.288</td>\n",
       "      <td>1400</td>\n",
       "      <td>1230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21605</th>\n",
       "      <td>3448900210</td>\n",
       "      <td>20141014T000000</td>\n",
       "      <td>610685.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2520</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5137</td>\n",
       "      <td>-122.167</td>\n",
       "      <td>2520</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21606</th>\n",
       "      <td>7936000429</td>\n",
       "      <td>20150326T000000</td>\n",
       "      <td>1007500.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3510</td>\n",
       "      <td>7200</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2600</td>\n",
       "      <td>910</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5537</td>\n",
       "      <td>-122.398</td>\n",
       "      <td>2050</td>\n",
       "      <td>6200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21607</th>\n",
       "      <td>2997800021</td>\n",
       "      <td>20150219T000000</td>\n",
       "      <td>475000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1310</td>\n",
       "      <td>1294</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1180</td>\n",
       "      <td>130</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98116</td>\n",
       "      <td>47.5773</td>\n",
       "      <td>-122.409</td>\n",
       "      <td>1330</td>\n",
       "      <td>1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date      price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000   221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000   538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000   180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000   604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000   510000.0         3       2.00   \n",
       "5      7237550310  20140512T000000  1225000.0         4       4.50   \n",
       "6      1321400060  20140627T000000   257500.0         3       2.25   \n",
       "7      2008000270  20150115T000000   291850.0         3       1.50   \n",
       "8      2414600126  20150415T000000   229500.0         3       1.00   \n",
       "9      3793500160  20150312T000000   323000.0         3       2.50   \n",
       "10     1736800520  20150403T000000   662500.0         3       2.50   \n",
       "11     9212900260  20140527T000000   468000.0         2       1.00   \n",
       "12      114101516  20140528T000000   310000.0         3       1.00   \n",
       "13     6054650070  20141007T000000   400000.0         3       1.75   \n",
       "14     1175000570  20150312T000000   530000.0         5       2.00   \n",
       "15     9297300055  20150124T000000   650000.0         4       3.00   \n",
       "16     1875500060  20140731T000000   395000.0         3       2.00   \n",
       "17     6865200140  20140529T000000   485000.0         4       1.00   \n",
       "18       16000397  20141205T000000   189000.0         2       1.00   \n",
       "19     7983200060  20150424T000000   230000.0         3       1.00   \n",
       "20     6300500875  20140514T000000   385000.0         4       1.75   \n",
       "21     2524049179  20140826T000000  2000000.0         3       2.75   \n",
       "22     7137970340  20140703T000000   285000.0         5       2.50   \n",
       "23     8091400200  20140516T000000   252700.0         2       1.50   \n",
       "24     3814700200  20141120T000000   329000.0         3       2.25   \n",
       "25     1202000200  20141103T000000   233000.0         3       2.00   \n",
       "26     1794500383  20140626T000000   937000.0         3       1.75   \n",
       "27     3303700376  20141201T000000   667000.0         3       1.00   \n",
       "28     5101402488  20140624T000000   438000.0         3       1.75   \n",
       "29     1873100390  20150302T000000   719000.0         4       2.50   \n",
       "...           ...              ...        ...       ...        ...   \n",
       "21583  2025049203  20140610T000000   399950.0         2       1.00   \n",
       "21584   952006823  20141202T000000   380000.0         3       2.50   \n",
       "21585  3832050760  20140828T000000   270000.0         3       2.50   \n",
       "21586  2767604724  20141015T000000   505000.0         2       2.50   \n",
       "21587  6632300207  20150305T000000   385000.0         3       2.50   \n",
       "21588  2767600688  20141113T000000   414500.0         2       1.50   \n",
       "21589  7570050450  20140910T000000   347500.0         3       2.50   \n",
       "21590  7430200100  20140514T000000  1222500.0         4       3.50   \n",
       "21591  4140940150  20141002T000000   572000.0         4       2.75   \n",
       "21592  1931300412  20150416T000000   475000.0         3       2.25   \n",
       "21593  8672200110  20150317T000000  1088000.0         5       3.75   \n",
       "21594  5087900040  20141017T000000   350000.0         4       2.75   \n",
       "21595  1972201967  20141031T000000   520000.0         2       2.25   \n",
       "21596  7502800100  20140813T000000   679950.0         5       2.75   \n",
       "21597   191100405  20150421T000000  1575000.0         4       3.25   \n",
       "21598  8956200760  20141013T000000   541800.0         4       2.50   \n",
       "21599  7202300110  20140915T000000   810000.0         4       3.00   \n",
       "21600   249000205  20141015T000000  1537000.0         5       3.75   \n",
       "21601  5100403806  20150407T000000   467000.0         3       2.50   \n",
       "21602   844000965  20140626T000000   224000.0         3       1.75   \n",
       "21603  7852140040  20140825T000000   507250.0         3       2.50   \n",
       "21604  9834201367  20150126T000000   429000.0         3       2.00   \n",
       "21605  3448900210  20141014T000000   610685.0         4       2.50   \n",
       "21606  7936000429  20150326T000000  1007500.0         4       3.50   \n",
       "21607  2997800021  20150219T000000   475000.0         3       2.50   \n",
       "21608   263000018  20140521T000000   360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000   400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000   402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000   400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000   325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view     ...      grade  \\\n",
       "0             1180      5650     1.0           0     0     ...          7   \n",
       "1             2570      7242     2.0           0     0     ...          7   \n",
       "2              770     10000     1.0           0     0     ...          6   \n",
       "3             1960      5000     1.0           0     0     ...          7   \n",
       "4             1680      8080     1.0           0     0     ...          8   \n",
       "5             5420    101930     1.0           0     0     ...         11   \n",
       "6             1715      6819     2.0           0     0     ...          7   \n",
       "7             1060      9711     1.0           0     0     ...          7   \n",
       "8             1780      7470     1.0           0     0     ...          7   \n",
       "9             1890      6560     2.0           0     0     ...          7   \n",
       "10            3560      9796     1.0           0     0     ...          8   \n",
       "11            1160      6000     1.0           0     0     ...          7   \n",
       "12            1430     19901     1.5           0     0     ...          7   \n",
       "13            1370      9680     1.0           0     0     ...          7   \n",
       "14            1810      4850     1.5           0     0     ...          7   \n",
       "15            2950      5000     2.0           0     3     ...          9   \n",
       "16            1890     14040     2.0           0     0     ...          7   \n",
       "17            1600      4300     1.5           0     0     ...          7   \n",
       "18            1200      9850     1.0           0     0     ...          7   \n",
       "19            1250      9774     1.0           0     0     ...          7   \n",
       "20            1620      4980     1.0           0     0     ...          7   \n",
       "21            3050     44867     1.0           0     4     ...          9   \n",
       "22            2270      6300     2.0           0     0     ...          8   \n",
       "23            1070      9643     1.0           0     0     ...          7   \n",
       "24            2450      6500     2.0           0     0     ...          8   \n",
       "25            1710      4697     1.5           0     0     ...          6   \n",
       "26            2450      2691     2.0           0     0     ...          8   \n",
       "27            1400      1581     1.5           0     0     ...          8   \n",
       "28            1520      6380     1.0           0     0     ...          7   \n",
       "29            2570      7173     2.0           0     0     ...          8   \n",
       "...            ...       ...     ...         ...   ...     ...        ...   \n",
       "21583          710      1157     2.0           0     0     ...          7   \n",
       "21584         1260       900     2.0           0     0     ...          7   \n",
       "21585         1870      5000     2.0           0     0     ...          7   \n",
       "21586         1430      1201     3.0           0     0     ...          8   \n",
       "21587         1520      1488     3.0           0     0     ...          8   \n",
       "21588         1210      1278     2.0           0     0     ...          8   \n",
       "21589         2540      4760     2.0           0     0     ...          8   \n",
       "21590         4910      9444     1.5           0     0     ...         11   \n",
       "21591         2770      3852     2.0           0     0     ...          8   \n",
       "21592         1190      1200     3.0           0     0     ...          8   \n",
       "21593         4170      8142     2.0           0     2     ...         10   \n",
       "21594         2500      5995     2.0           0     0     ...          8   \n",
       "21595         1530       981     3.0           0     0     ...          8   \n",
       "21596         3600      9437     2.0           0     0     ...          9   \n",
       "21597         3410     10125     2.0           0     0     ...         10   \n",
       "21598         3118      7866     2.0           0     2     ...          9   \n",
       "21599         3990      7838     2.0           0     0     ...          9   \n",
       "21600         4470      8088     2.0           0     0     ...         11   \n",
       "21601         1425      1179     3.0           0     0     ...          8   \n",
       "21602         1500     11968     1.0           0     0     ...          6   \n",
       "21603         2270      5536     2.0           0     0     ...          8   \n",
       "21604         1490      1126     3.0           0     0     ...          8   \n",
       "21605         2520      6023     2.0           0     0     ...          9   \n",
       "21606         3510      7200     2.0           0     0     ...          9   \n",
       "21607         1310      1294     2.0           0     0     ...          8   \n",
       "21608         1530      1131     3.0           0     0     ...          8   \n",
       "21609         2310      5813     2.0           0     0     ...          8   \n",
       "21610         1020      1350     2.0           0     0     ...          7   \n",
       "21611         1600      2388     2.0           0     0     ...          8   \n",
       "21612         1020      1076     2.0           0     0     ...          7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "5            3890           1530      2001             0    98053  47.6561   \n",
       "6            1715              0      1995             0    98003  47.3097   \n",
       "7            1060              0      1963             0    98198  47.4095   \n",
       "8            1050            730      1960             0    98146  47.5123   \n",
       "9            1890              0      2003             0    98038  47.3684   \n",
       "10           1860           1700      1965             0    98007  47.6007   \n",
       "11            860            300      1942             0    98115  47.6900   \n",
       "12           1430              0      1927             0    98028  47.7558   \n",
       "13           1370              0      1977             0    98074  47.6127   \n",
       "14           1810              0      1900             0    98107  47.6700   \n",
       "15           1980            970      1979             0    98126  47.5714   \n",
       "16           1890              0      1994             0    98019  47.7277   \n",
       "17           1600              0      1916             0    98103  47.6648   \n",
       "18           1200              0      1921             0    98002  47.3089   \n",
       "19           1250              0      1969             0    98003  47.3343   \n",
       "20            860            760      1947             0    98133  47.7025   \n",
       "21           2330            720      1968             0    98040  47.5316   \n",
       "22           2270              0      1995             0    98092  47.3266   \n",
       "23           1070              0      1985             0    98030  47.3533   \n",
       "24           2450              0      1985             0    98030  47.3739   \n",
       "25           1710              0      1941             0    98002  47.3048   \n",
       "26           1750            700      1915             0    98119  47.6386   \n",
       "27           1400              0      1909             0    98112  47.6221   \n",
       "28            790            730      1948             0    98115  47.6950   \n",
       "29           2570              0      2005             0    98052  47.7073   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21583         710              0      1943             0    98102  47.6413   \n",
       "21584         940            320      2007             0    98116  47.5621   \n",
       "21585        1870              0      2009             0    98042  47.3339   \n",
       "21586        1430              0      2009             0    98107  47.6707   \n",
       "21587        1520              0      2006             0    98125  47.7337   \n",
       "21588        1020            190      2007             0    98117  47.6756   \n",
       "21589        2540              0      2010             0    98038  47.3452   \n",
       "21590        3110           1800      2007             0    98074  47.6502   \n",
       "21591        2770              0      2014             0    98178  47.5001   \n",
       "21592        1190              0      2008             0    98103  47.6542   \n",
       "21593        4170              0      2006             0    98056  47.5354   \n",
       "21594        2500              0      2008             0    98042  47.3749   \n",
       "21595        1480             50      2006             0    98103  47.6533   \n",
       "21596        3600              0      2014             0    98059  47.4822   \n",
       "21597        3410              0      2007             0    98040  47.5653   \n",
       "21598        3118              0      2014             0    98001  47.2931   \n",
       "21599        3990              0      2003             0    98053  47.6857   \n",
       "21600        4470              0      2008             0    98004  47.6321   \n",
       "21601        1425              0      2008             0    98125  47.6963   \n",
       "21602        1500              0      2014             0    98010  47.3095   \n",
       "21603        2270              0      2003             0    98065  47.5389   \n",
       "21604        1490              0      2014             0    98144  47.5699   \n",
       "21605        2520              0      2014             0    98056  47.5137   \n",
       "21606        2600            910      2009             0    98136  47.5537   \n",
       "21607        1180            130      2008             0    98116  47.5773   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "5     -122.005           4760      101930  \n",
       "6     -122.327           2238        6819  \n",
       "7     -122.315           1650        9711  \n",
       "8     -122.337           1780        8113  \n",
       "9     -122.031           2390        7570  \n",
       "10    -122.145           2210        8925  \n",
       "11    -122.292           1330        6000  \n",
       "12    -122.229           1780       12697  \n",
       "13    -122.045           1370       10208  \n",
       "14    -122.394           1360        4850  \n",
       "15    -122.375           2140        4000  \n",
       "16    -121.962           1890       14018  \n",
       "17    -122.343           1610        4300  \n",
       "18    -122.210           1060        5095  \n",
       "19    -122.306           1280        8850  \n",
       "20    -122.341           1400        4980  \n",
       "21    -122.233           4110       20336  \n",
       "22    -122.169           2240        7005  \n",
       "23    -122.166           1220        8386  \n",
       "24    -122.172           2200        6865  \n",
       "25    -122.218           1030        4705  \n",
       "26    -122.360           1760        3573  \n",
       "27    -122.314           1860        3861  \n",
       "28    -122.304           1520        6235  \n",
       "29    -122.110           2630        6026  \n",
       "...        ...            ...         ...  \n",
       "21583 -122.329           1370        1173  \n",
       "21584 -122.384           1310        1415  \n",
       "21585 -122.055           2170        5399  \n",
       "21586 -122.381           1430        1249  \n",
       "21587 -122.309           1520        1497  \n",
       "21588 -122.375           1210        1118  \n",
       "21589 -122.022           2540        4571  \n",
       "21590 -122.066           4560       11063  \n",
       "21591 -122.232           1810        5641  \n",
       "21592 -122.346           1180        1224  \n",
       "21593 -122.181           3030        7980  \n",
       "21594 -122.107           2530        5988  \n",
       "21595 -122.346           1530        1282  \n",
       "21596 -122.131           3550        9421  \n",
       "21597 -122.223           2290       10125  \n",
       "21598 -122.264           2673        6500  \n",
       "21599 -122.046           3370        6814  \n",
       "21600 -122.200           2780        8964  \n",
       "21601 -122.318           1285        1253  \n",
       "21602 -122.002           1320       11303  \n",
       "21603 -121.881           2270        5731  \n",
       "21604 -122.288           1400        1230  \n",
       "21605 -122.167           2520        6023  \n",
       "21606 -122.398           2050        6200  \n",
       "21607 -122.409           1330        1265  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, you're better off using a simple model that gives a pretty good answer than a super complicated model that gives a slightly better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intro simple model - super similar to what we did in sklearn\n",
    "column_selection = [\"bedrooms\",\"bathrooms\",\"sqft_living\",\"sqft_lot\",\n",
    "                    \"floors\",\"condition\",\"grade\",\"sqft_above\",\n",
    "                    \"sqft_basement\",\"sqft_living15\",\"sqft_lot15\",\n",
    "                    \"lat\", \"long\",\"yr_built\",\"yr_renovated\",\"waterfront\"]\n",
    "\n",
    "selected_feature = np.array(data[column_selection])\n",
    "price = np.array(data[\"price\"])\n",
    "selected_feature_train = selected_feature[:20000]\n",
    "price_train = price[:20000]\n",
    "\n",
    "selected_feature_test = selected_feature[20000:]\n",
    "price_test = price[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y,y_pred):\n",
    "    return np.mean(np.abs(y-y_pred)/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "#currently, our model has nothing in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_len = len(column_selection) #how many features are in our vector\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation='relu')) #dense layer, number of nodes is 50, input is number of columns\n",
    "#relu is the activation function that's y= 0 for x < 0 and y = x for x>= 0\n",
    "model.add(keras.layers.Dense(50, activation='relu')) #no input size because it takes from the previous layer and infers\n",
    "model.add(keras.layers.Dense(1)) #just the output function - no activation function==linear activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Densely connected layer refers to the architecture - you're doing all possible combinations of matrix multiplication - 'true' matrix multiplication\n",
    "Basically, everything talks to each other\n",
    "Dense is the standard for most things\n",
    "Tomorrow we'll talk aout convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam') #adam is a pretty basic optimizer that a lot of people use\n",
    "#Notice that there's still no output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20000/20000 [==============================] - 1s 56us/step - loss: 361255999635.4560\n",
      "Epoch 2/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 224477150209.6384\n",
      "Epoch 3/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 99854205435.9040\n",
      "Epoch 4/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 76699223130.1120\n",
      "Epoch 5/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 74171156607.7952\n",
      "Epoch 6/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 72188685739.6224\n",
      "Epoch 7/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 70448329254.5024\n",
      "Epoch 8/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 68937378011.5456\n",
      "Epoch 9/50\n",
      "20000/20000 [==============================] - 0s 11us/step - loss: 67343642466.7136\n",
      "Epoch 10/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 66182002976.3584\n",
      "Epoch 11/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 65096396773.7856\n",
      "Epoch 12/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 64326967033.8560\n",
      "Epoch 13/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 63758254656.7168\n",
      "Epoch 14/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 63324605828.3008\n",
      "Epoch 15/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 62910318808.2688\n",
      "Epoch 16/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62752588726.2720\n",
      "Epoch 17/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 62525284050.5344\n",
      "Epoch 18/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62283604492.2880\n",
      "Epoch 19/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62316454320.5376\n",
      "Epoch 20/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 62037984044.6464\n",
      "Epoch 21/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61944443056.9472\n",
      "Epoch 22/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61528278879.4368\n",
      "Epoch 23/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 61396516156.2112\n",
      "Epoch 24/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61455710853.5296\n",
      "Epoch 25/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61187611328.5120\n",
      "Epoch 26/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 61012355894.4768\n",
      "Epoch 27/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60884454250.9056\n",
      "Epoch 28/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60996303886.7456\n",
      "Epoch 29/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60447741758.6688\n",
      "Epoch 30/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60149774929.1008\n",
      "Epoch 31/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 60038254932.7872\n",
      "Epoch 32/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 59918278354.5344\n",
      "Epoch 33/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 59962765410.3040\n",
      "Epoch 34/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 59482889086.5664\n",
      "Epoch 35/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 59544191470.7968\n",
      "Epoch 36/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 59337486761.9840\n",
      "Epoch 37/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 59190888572.5184\n",
      "Epoch 38/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 58938552732.8768\n",
      "Epoch 39/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 58557766087.4752\n",
      "Epoch 40/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 58593098694.6560\n",
      "Epoch 41/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 58583489649.0496\n",
      "Epoch 42/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 58265273961.6768\n",
      "Epoch 43/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 57995580283.2896\n",
      "Epoch 44/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 57991962624.0000\n",
      "Epoch 45/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 57717205434.3680\n",
      "Epoch 46/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 57577115929.8048\n",
      "Epoch 47/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 57371725830.5536\n",
      "Epoch 48/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 57283625549.8240\n",
      "Epoch 49/50\n",
      "20000/20000 [==============================] - 0s 10us/step - loss: 57210513208.1152\n",
      "Epoch 50/50\n",
      "20000/20000 [==============================] - 0s 9us/step - loss: 57081710850.8672\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we're actually fitting and using the model. We're hoping it will have compiled by 50 iterations, but we don't actually know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5865828178320837"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(selected_feature_test)\n",
    "score(preds, price_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like SKLearn, it's easy to train and evaluate simple models.\n",
    "#### ... but we should try to do better\n",
    "SKlearn is really good for deciding which models is the best to use, but we want to utilize more features if we know we want to go with neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Deep Learning -- What you need to know\n",
    "We divide our data into three sets:\n",
    "### Train, Validation, Test:\n",
    "   * Optimize parameters with Train (weights, biases)\n",
    "   * Optimize hyperparameters with Validation (layer width & depth, activation functions, etc.)\n",
    "    - things that can't be altered in line - have to make alterations to the code\n",
    "   * Optimize NOTHING with Test\n",
    "    - must hold out until the very end, otherwise it loses its value as a test. We want it to be similar to what our model will actually encounter once we release it \"into the wild\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out a validation set for hyperparameter optimization\n",
    "selected_feature_train = selected_feature[:18000]\n",
    "price_train = price[:18000]\n",
    "\n",
    "selected_feature_val = selected_feature[18000:20000]\n",
    "price_val = price[18000:20000]\n",
    "\n",
    "selected_feature_test[20000:]\n",
    "price_test = price[20000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, try better validation schemes like [k-fold cross validation](https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/), though 80/20 or 90/10 train/val like this works in a pinch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a hyperparameter optimization:\n",
    "\n",
    "### Try three activation functions to use for dense layers in the neural network above. Save the model that achieves the best validation loss "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint: [activation functions](http://letmegooglethat.com/?q=keras+activation+functions)\n",
    "\n",
    "#### Hint: `model.fit` has argument \"`validation_data`\" which takes a tuple of features and targets\n",
    "\n",
    "#### Hint: Use `model.save(\"filename.h5\")` to save a model locally. If you want to use it later, just call `keras.models.load_model(\"filename.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 0s 24us/step - loss: 416674509692.0178\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416674154574.2791\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416673798495.3458\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416673427503.3316\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416673046841.1165\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416672659741.8098\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416672276050.3751\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416671889620.9920\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416671483152.1565\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416671049303.8364\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416670588775.0827\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416670117789.6960\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416669648610.1902\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416669176051.9396\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416668709173.4755\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416668266790.9120\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416667856186.0267\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416667473135.3884\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416667108172.6862\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416666754394.7947\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416666411190.0445\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416666078878.8338\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416665747091.9111\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416665424101.3760\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416665103324.5013\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416664786683.6765\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416664469285.5467\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416664154974.8906\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416663847829.5040\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416663536344.1778\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416663223722.8942\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416662919140.6933\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416662611004.9849\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416662312160.8249\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416662001403.6765\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416661702443.0080\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416661392676.1813\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416661091268.8356\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416660788521.6426\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416660484929.7635\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416660189376.9671\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416659880396.5724\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416659581348.5227\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416659278251.8044\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416658978475.5769\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416658676165.2906\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416658376301.6818\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416658079467.2924\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416657774156.9138\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416657473623.3813\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "input_len = len(column_selection)\n",
    "model.add(keras.layers.Dense(50, input_dim=input_len, activation='softmax'))\n",
    "model.add(keras.layers.Dense(50, activation='softmax'))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For easy looping, define neural network model as a function\n",
    "def nn_model(optimizer='adam', \n",
    "             activation='relu',\n",
    "             nodes=50,\n",
    "             loss='mean_squared_error'):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(nodes, activation=activation))\n",
    "    model.add(keras.layers.Dense(nodes, activation=activation))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 0s 27us/step - loss: 416674540479.3742 - val_loss: 456363804721.1520\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416674181079.9502 - val_loss: 456363407310.8480\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416673827535.0756 - val_loss: 456363025367.0399\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416673457970.2898 - val_loss: 456362649714.6880\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416673085434.5386 - val_loss: 456362250993.6639\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416672702325.6462 - val_loss: 456361866952.7040\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416672321197.3973 - val_loss: 456361454338.0480\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416671926903.6943 - val_loss: 456361027043.3280\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416671512978.3182 - val_loss: 456360567767.0399\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416671057139.0293 - val_loss: 456360089616.3840\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416670576279.5520 - val_loss: 456359556415.4880\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416670069759.0898 - val_loss: 456359033700.3519\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416669562044.4160 - val_loss: 456358489227.2640\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416669063126.1298 - val_loss: 456358021038.0800\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416668606442.1547 - val_loss: 456357545508.8641\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416668192691.5413 - val_loss: 456357127389.1840\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416667805213.5823 - val_loss: 456356740464.6400\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416667429735.9929 - val_loss: 456356341743.6160\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416667082744.7182 - val_loss: 456355997548.5441\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416666731850.4106 - val_loss: 456355628711.9359\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416666399597.4542 - val_loss: 456355282944.0000\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416666072820.3947 - val_loss: 456354947137.5360\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416665742635.4631 - val_loss: 456354619457.5360\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416665424887.8080 - val_loss: 456354273165.3120\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416665106528.4836 - val_loss: 456353919008.7679\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416664790062.4213 - val_loss: 456353608892.4160\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416664478169.3156 - val_loss: 456353266794.4960\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416664164149.9307 - val_loss: 456352959299.5840\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416663854586.9938 - val_loss: 456352633192.4481\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416663548723.2000 - val_loss: 456352304201.7280\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416663238985.5004 - val_loss: 456351998803.9680\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416662929859.4702 - val_loss: 456351647531.0080\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416662624549.0916 - val_loss: 456351335841.7920\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416662323141.7458 - val_loss: 456351046696.9601\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416662015705.0880 - val_loss: 456350708006.9120\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416661715579.3351 - val_loss: 456350395793.4080\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416661408492.2026 - val_loss: 456350082007.0399\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416661106181.9164 - val_loss: 456349779492.8641\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416660807396.0107 - val_loss: 456349441589.2479\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416660501211.8187 - val_loss: 456349131997.1840\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416660203212.3449 - val_loss: 456348807987.2000\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416659899678.7200 - val_loss: 456348483190.7840\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416659599436.4587 - val_loss: 456348179890.1760\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416659293077.5040 - val_loss: 456347855880.1920\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416658994903.2675 - val_loss: 456347544190.9760\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416658689505.5076 - val_loss: 456347219394.5599\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416658394156.6009 - val_loss: 456346914783.2321\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416658091700.6791 - val_loss: 456346587889.6639\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416657789827.2996 - val_loss: 456346298744.8319\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416657492992.9102 - val_loss: 456345979453.4401\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 0s 27us/step - loss: 416669557442.3325 - val_loss: 456354094907.3920\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416661642703.3031 - val_loss: 456346925268.9920\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416655208437.0773 - val_loss: 456340461322.2401\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416648795433.6426 - val_loss: 456333531021.3120\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416642337953.1094 - val_loss: 456327004946.4320\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416636203055.3316 - val_loss: 456320654508.0320\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416630194190.5635 - val_loss: 456314424131.5840\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416624272153.7137 - val_loss: 456308245135.3600\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416618396487.2249 - val_loss: 456302115684.3519\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416612556647.0827 - val_loss: 456296012185.6000\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416606391952.2703 - val_loss: 456289315979.2640\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416600197285.6605 - val_loss: 456282964754.4320\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416594137506.7022 - val_loss: 456276622180.3519\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416587993200.8676 - val_loss: 456269865418.7521\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416581466747.3351 - val_loss: 456263192281.0880\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416575175087.4454 - val_loss: 456256686391.2960\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416568984760.7751 - val_loss: 456250230046.7199\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416562842726.8551 - val_loss: 456243810140.1600\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416556721664.4551 - val_loss: 456237442924.5441\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416550634272.9956 - val_loss: 456231062339.5840\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416544555444.9067 - val_loss: 456224689094.6561\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416538494326.1013 - val_loss: 456218359627.7760\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416532439615.2604 - val_loss: 456212021248.0000\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416526393992.0782 - val_loss: 456205715374.0800\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416520356903.1395 - val_loss: 456199384334.3361\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416514325231.8435 - val_loss: 456193085014.0160\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416508296939.2924 - val_loss: 456186756857.8561\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416502274297.4009 - val_loss: 456180471955.4559\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416496259082.9227 - val_loss: 456174165557.2479\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416490237897.3867 - val_loss: 456167878557.6960\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416484229061.7458 - val_loss: 456161583169.5360\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416478215274.4960 - val_loss: 456155289354.2401\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416472205565.0417 - val_loss: 456148996063.2321\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416466197370.1973 - val_loss: 456142692286.4640\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416460188301.5396 - val_loss: 456136432549.8880\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416454185174.8124 - val_loss: 456130152628.2240\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416448180009.1876 - val_loss: 456123846754.3040\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416442176212.5369 - val_loss: 456117553463.2960\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416436172911.0471 - val_loss: 456111270658.0480\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416430168444.4729 - val_loss: 456104994144.2560\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416424164735.2036 - val_loss: 456098722611.2000\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416418164841.5858 - val_loss: 456092431417.3439\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416412163142.0871 - val_loss: 456086152806.4000\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416406161588.2240 - val_loss: 456079870001.1520\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416400158607.1324 - val_loss: 456073580904.4481\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416394157839.7014 - val_loss: 456067294953.4720\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416388156897.5076 - val_loss: 456061012672.5120\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416382160033.1094 - val_loss: 456054743236.6080\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416376163547.3635 - val_loss: 456048443654.1439\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416370163508.1102 - val_loss: 456042197024.7679\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 30us/step - loss: 416662995075.0720 - val_loss: 456345610878.9760\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416652813693.3831 - val_loss: 456336936271.8719\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416644732201.6426 - val_loss: 456328600879.1040\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416636835783.5662 - val_loss: 456320379256.8319\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416629025756.5013 - val_loss: 456312228413.4401\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416621258779.3067 - val_loss: 456304133406.7199\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416613518948.5795 - val_loss: 456296027389.9520\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416605796972.7715 - val_loss: 456287958073.3439\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416598086181.7742 - val_loss: 456279904223.2321\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416590388701.8666 - val_loss: 456271819702.2720\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416582704300.0320 - val_loss: 456263779745.7920\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416575020801.1378 - val_loss: 456255738478.5920\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416567343855.8435 - val_loss: 456247713202.1760\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416559668891.1929 - val_loss: 456239698411.5200\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416552005256.9885 - val_loss: 456231657668.6080\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416544337282.8445 - val_loss: 456223638159.3600\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416536675978.8089 - val_loss: 456215616290.8160\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416529012927.1467 - val_loss: 456207602810.8800\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416521350691.0435 - val_loss: 456199593525.2479\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416513689620.0249 - val_loss: 456191553568.7679\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 416506033733.6320 - val_loss: 456183536680.9601\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416498376449.1378 - val_loss: 456175525298.1760\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416490726533.8027 - val_loss: 456167520206.8480\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416483072395.0364 - val_loss: 456159517212.6720\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416475416333.8809 - val_loss: 456151491411.9680\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416467764204.8853 - val_loss: 456143493660.6720\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416460111318.5849 - val_loss: 456135465762.8160\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416452465655.8080 - val_loss: 456127450185.7280\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416444811895.6943 - val_loss: 456119463968.7679\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416437167485.3831 - val_loss: 456111465168.8960\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416429517366.1583 - val_loss: 456103431503.8719\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416421870654.8053 - val_loss: 456095441092.6080\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416414224729.8845 - val_loss: 456087445176.3200\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416406580406.9547 - val_loss: 456079402336.2560\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416398929093.5182 - val_loss: 456071407730.6880\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416391285324.0035 - val_loss: 456063423610.8800\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416383643069.0987 - val_loss: 456055412228.0960\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416375996619.8898 - val_loss: 456047416836.0960\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416368350258.0622 - val_loss: 456039398637.5680\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416360706459.4205 - val_loss: 456031429722.1120\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416353060126.7200 - val_loss: 456023400251.3920\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416345410386.1476 - val_loss: 456015405645.8240\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416337761985.4222 - val_loss: 456007411564.5441\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416330115099.3067 - val_loss: 455999382880.2560\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416322466203.4205 - val_loss: 455991390896.1281\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 416314818967.7795 - val_loss: 455983369289.7280\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416307175809.9343 - val_loss: 455975393558.5280\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416299531224.8605 - val_loss: 455967354912.7679\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416291882154.2115 - val_loss: 455959388880.8960\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 416284233170.9440 - val_loss: 455951377498.1120\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 30us/step - loss: 365288805498.8800 - val_loss: 337213268951.0400\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 262795309634.9013 - val_loss: 225864633417.7280\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 138226469925.3191 - val_loss: 98639104507.9040\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 79568759288.2631 - val_loss: 83857850105.8560\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 74832430148.2667 - val_loss: 81618170871.8080\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 72573216622.3645 - val_loss: 79507253493.7600\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 70827357874.8587 - val_loss: 78649090310.1440\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 69289647567.3031 - val_loss: 77053739466.7520\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 68047410713.9413 - val_loss: 75704078827.5200\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 66944282627.6409 - val_loss: 74796154880.0000\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 65992991626.5813 - val_loss: 74035281657.8560\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 65277966574.4782 - val_loss: 73266202673.1520\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 64753380777.0738 - val_loss: 72787664568.3200\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 64106239499.3778 - val_loss: 72559263285.2480\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63978136669.7529 - val_loss: 72346864713.7280\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 63472533037.9662 - val_loss: 72226034089.9840\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63361176036.2382 - val_loss: 71957968977.9200\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63523542062.4213 - val_loss: 71918372323.3280\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 63307575474.4036 - val_loss: 72222318264.3200\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63140727262.7769 - val_loss: 71906004828.1600\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 63013007214.3644 - val_loss: 72059790032.8960\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63018774247.6516 - val_loss: 71788463259.6480\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62993447836.7858 - val_loss: 72253673832.4480\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63268494129.3795 - val_loss: 72027250229.2480\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 63234418751.7156 - val_loss: 71935234539.5200\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 63374348561.0667 - val_loss: 72296615641.0880\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 62892301298.3467 - val_loss: 72159805440.0000\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 62761960953.1733 - val_loss: 72003663495.1680\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62821196205.6249 - val_loss: 72122623983.6160\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62820043176.1636 - val_loss: 71863895719.9360\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 62603107216.0427 - val_loss: 72369687166.9760\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62644776643.2427 - val_loss: 72317916020.7360\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62710452253.1271 - val_loss: 71990234251.2640\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62685363328.3413 - val_loss: 71844119969.7920\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 62799526304.8818 - val_loss: 71715498360.8320\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 62410374982.3147 - val_loss: 72071379812.3520\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62344452767.7440 - val_loss: 72351091720.1920\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 62467880943.6160 - val_loss: 71992255053.8240\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62274755832.4907 - val_loss: 71751199031.2960\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62200765721.2587 - val_loss: 71747168043.0080\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 61977416175.1609 - val_loss: 71581638000.6400\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 62050436106.0124 - val_loss: 71678596874.2400\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 61742801915.4489 - val_loss: 71575114350.5920\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 61708499258.9369 - val_loss: 71725142376.4480\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 61544568527.9858 - val_loss: 71395699720.1920\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 61739771883.9751 - val_loss: 71563052187.6480\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 61328126661.9733 - val_loss: 71559213416.4480\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 61342388504.3484 - val_loss: 71366949208.0640\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 61149711309.9378 - val_loss: 71037830168.5760\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 61037411944.2204 - val_loss: 71261062430.7200\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 33us/step - loss: 364981919911.4808 - val_loss: 341931576524.8000\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 261477057312.9956 - val_loss: 214627627302.9120\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 125042716959.6302 - val_loss: 91099602878.4640\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 77257713501.9805 - val_loss: 83313550557.1840\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 74539015427.4133 - val_loss: 81824685359.1040\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 72409649938.4320 - val_loss: 79996883632.1280\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 70851099015.3956 - val_loss: 78945082671.1040\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 69329888965.0631 - val_loss: 77035321688.0640\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 67966285439.8862 - val_loss: 76130914795.5200\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 66832869607.1964 - val_loss: 74952858271.7440\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 66015006883.8400 - val_loss: 74125938262.0160\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 65356118946.2471 - val_loss: 73587675824.1280\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 64582007921.7778 - val_loss: 73034642292.7360\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 64152746277.0916 - val_loss: 72774473613.3120\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 63712442941.4400 - val_loss: 72597054881.7920\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63596994039.3529 - val_loss: 72179889405.9520\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63680079440.5547 - val_loss: 71984339222.5280\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 63110518217.8418 - val_loss: 71985520902.1440\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 63146701160.4480 - val_loss: 72048098607.1040\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62859358757.7742 - val_loss: 71974466093.0560\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62792378621.0418 - val_loss: 71995474968.5760\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 62827459433.8133 - val_loss: 71787004493.8240\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62666635841.0809 - val_loss: 71632861462.5280\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62507243955.0862 - val_loss: 72267719180.2880\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62487057115.8187 - val_loss: 71388818702.3360\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62359819503.3884 - val_loss: 71658507468.8000\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 62086522433.9911 - val_loss: 71285916106.7520\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 61712539582.4640 - val_loss: 70987102814.2080\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 61650527587.8969 - val_loss: 71546015383.5520\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 61393986213.2053 - val_loss: 70710417948.6720\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 61269576537.4293 - val_loss: 71463982333.9520\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 61243387703.7511 - val_loss: 70382456995.8400\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 60808686694.8551 - val_loss: 70260012417.0240\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 60713253411.9538 - val_loss: 70028902662.1440\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 60487819358.6631 - val_loss: 69810594840.5760\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 60260469267.5698 - val_loss: 69693326884.8640\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 60309413898.9227 - val_loss: 69699946283.0080\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 59832153429.3333 - val_loss: 69412445683.7120\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 59662461042.6880 - val_loss: 69044254736.3840\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 59749949571.0720 - val_loss: 68938642948.0960\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 59129969501.0702 - val_loss: 68586990141.4400\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 58891910101.2196 - val_loss: 68613597429.7600\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 59071823420.5298 - val_loss: 67968016056.3200\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 58492242063.8151 - val_loss: 68005916704.7680\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 58143429751.2391 - val_loss: 67479379902.4640\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 58212134260.2809 - val_loss: 67375877259.2640\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 57767936828.3022 - val_loss: 67118089830.4000\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 57533555614.6062 - val_loss: 66847670927.3600\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 57288103455.4027 - val_loss: 67052680577.0240\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 57158218365.1556 - val_loss: 66742541025.2800\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/50\n",
      "18000/18000 [==============================] - 1s 35us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/50\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/50\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/50\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/50\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: nan - val_loss: nan\n",
      "BEST ACTIVATION FUNCTION softplus WITH SCORE 0.6688727641618445\n"
     ]
    }
   ],
   "source": [
    "best_score = 1000.0 #bad\n",
    "\n",
    "#loop over activation functions, test on valid, take model with best results\n",
    "for activ in ['softmax', 'sigmoid', 'tanh', 'relu', 'softplus', 'exponential']:\n",
    "    model = nn_model(activation=activ)\n",
    "    \n",
    "    history = model.fit(selected_feature_train, price_train,\n",
    "                       epochs=50, batch_size=128,\n",
    "                       validation_data=(selected_feature_val, price_val))\n",
    "    model_score = score(model.predict(selected_feature_val), price_val)\n",
    "    \n",
    "    if model_score < best_score:\n",
    "        best_score = model_score\n",
    "        best_activ = activ\n",
    "        best_model = model\n",
    "        best_train = history\n",
    "        \n",
    "print(f\"BEST ACTIVATION FUNCTION {best_activ} WITH SCORE {best_score}\")\n",
    "best_model.save(\"awesome_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize your training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot loss during training\n",
    "def plot_loss(hist):\n",
    "    %matplotlib inline\n",
    "    plt.title('Training Curve')\n",
    "    plt.plot(hist.history['loss'], label='train')\n",
    "    plt.plot(hist.history['val_loss'], label='validation')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Mean squared error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_loss(best_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Standardize your features:\n",
    "* Typically assumes normally distributed feature, shifting mean to 0 and standard deviation to 1\n",
    "  - makes it so all features look and are scaled the same way\n",
    "* In theory does not matter for neural networks\n",
    "* In practice tends to matter for neural networks\n",
    "* Scale if using:\n",
    "    - Logistic regression\n",
    "    - Support vector machines\n",
    "    - Perceptrons\n",
    "    - Neural networks\n",
    "    - Principle component analysis\n",
    "* Don't bother if using:\n",
    "    - \"Forest\" methods\n",
    "    - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.5395092608135503e-17\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "in_scaler = StandardScaler()\n",
    "\n",
    "selected_feature_train = in_scaler.fit_transform(selected_feature_train)\n",
    "\n",
    "selected_feature_val = in_scaler.fit_transform(selected_feature_val)\n",
    "selected_feature_test = in_scaler.fit_transform(selected_feature_test)\n",
    "#we want to use the same scale - could get different values for shift and division\n",
    "# if we re-scaled for validation and test\n",
    "\n",
    "print(np.mean(selected_feature_train[:,0]))\n",
    "print(np.std(selected_feature_train[:,0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "18000/18000 [==============================] - 1s 36us/step - loss: 416642480064.2845 - val_loss: 456229088657.4080\n",
      "Epoch 2/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416148003953.7778 - val_loss: 455136606945.2801\n",
      "Epoch 3/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 414126007764.7645 - val_loss: 451867020886.0160\n",
      "Epoch 4/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 409518329823.2320 - val_loss: 445473458487.2960\n",
      "Epoch 5/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 401489618534.4000 - val_loss: 435227743223.8080\n",
      "Epoch 6/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 389485163621.0347 - val_loss: 420833394425.8560\n",
      "Epoch 7/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 373262962363.9609 - val_loss: 402034979766.2720\n",
      "Epoch 8/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 353273364203.2924 - val_loss: 379393502085.1200\n",
      "Epoch 9/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 330030387662.3929 - val_loss: 353622085337.0880\n",
      "Epoch 10/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 303905424539.6480 - val_loss: 325239312482.3040\n",
      "Epoch 11/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 275884256540.8995 - val_loss: 295250335105.0240\n",
      "Epoch 12/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 247422444998.2009 - val_loss: 265006251769.8560\n",
      "Epoch 13/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 219779732602.8800 - val_loss: 236083024756.7360\n",
      "Epoch 14/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 194124482725.6605 - val_loss: 209173991260.1600\n",
      "Epoch 15/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 171202131081.4435 - val_loss: 185475396403.2000\n",
      "Epoch 16/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 151469028423.9076 - val_loss: 164819059081.2160\n",
      "Epoch 17/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 135062519778.8729 - val_loss: 147474415747.0720\n",
      "Epoch 18/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 121825302923.0365 - val_loss: 133389467123.7120\n",
      "Epoch 19/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 111393588306.8302 - val_loss: 122209807237.1200\n",
      "Epoch 20/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 103233639750.7698 - val_loss: 113122190884.8640\n",
      "Epoch 21/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 96789533913.5431 - val_loss: 106005940731.9040\n",
      "Epoch 22/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 91638086864.4409 - val_loss: 100187318059.0080\n",
      "Epoch 23/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 87412870166.7556 - val_loss: 95599349989.3760\n",
      "Epoch 24/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 83884041925.0631 - val_loss: 91768392187.9040\n",
      "Epoch 25/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 80913218528.1422 - val_loss: 88591615197.1840\n",
      "Epoch 26/100\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 78400341395.2284 - val_loss: 85952151420.9280\n",
      "Epoch 27/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 76242998318.4213 - val_loss: 83749453430.7840\n",
      "Epoch 28/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 74404348974.4213 - val_loss: 81820839510.0160\n",
      "Epoch 29/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 72809411623.1396 - val_loss: 80206886928.3840\n",
      "Epoch 30/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 71444294216.3627 - val_loss: 78915985604.6080\n",
      "Epoch 31/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 70255500671.2036 - val_loss: 77700496293.8880\n",
      "Epoch 32/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 69186744032.3698 - val_loss: 76525626982.4000\n",
      "Epoch 33/100\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 68213537155.7547 - val_loss: 75678667374.5920\n",
      "Epoch 34/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 67325141181.3262 - val_loss: 74629474287.6160\n",
      "Epoch 35/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 66478210718.8338 - val_loss: 73818910687.2320\n",
      "Epoch 36/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 65665857231.9858 - val_loss: 72960398196.7360\n",
      "Epoch 37/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 64878761483.3778 - val_loss: 72182713417.7280\n",
      "Epoch 38/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 64126645647.5876 - val_loss: 71333849071.6160\n",
      "Epoch 39/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63374810174.8053 - val_loss: 70480075948.0320\n",
      "Epoch 40/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62633708402.9156 - val_loss: 69718664740.8640\n",
      "Epoch 41/100\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 61891220628.3662 - val_loss: 68875035115.5200\n",
      "Epoch 42/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 61158883371.6907 - val_loss: 68107095506.9440\n",
      "Epoch 43/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 60439778936.6044 - val_loss: 67248310190.0800\n",
      "Epoch 44/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 59700232264.8178 - val_loss: 66556245278.7200\n",
      "Epoch 45/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 58974840298.6098 - val_loss: 65848248565.7600\n",
      "Epoch 46/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 58255163544.9173 - val_loss: 65066672553.9840\n",
      "Epoch 47/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 57527809223.3387 - val_loss: 64269371899.9040\n",
      "Epoch 48/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 56815147406.6773 - val_loss: 63467367301.1200\n",
      "Epoch 49/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 56096998016.7964 - val_loss: 62782612406.2720\n",
      "Epoch 50/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 55374817525.7600 - val_loss: 62022428950.5280\n",
      "Epoch 51/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 54665071667.8827 - val_loss: 61271837966.3360\n",
      "Epoch 52/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 53955656249.7991 - val_loss: 60581170118.6560\n",
      "Epoch 53/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 53251554206.6062 - val_loss: 59902776868.8640\n",
      "Epoch 54/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 52541387512.9458 - val_loss: 59115705565.1840\n",
      "Epoch 55/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 51860077237.5893 - val_loss: 58467235594.2400\n",
      "Epoch 56/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 51166060336.4693 - val_loss: 57450505666.5600\n",
      "Epoch 57/100\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 50478150028.8569 - val_loss: 56859447099.3920\n",
      "Epoch 58/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 49807265977.6853 - val_loss: 56180362444.8000\n",
      "Epoch 59/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 49150582401.7067 - val_loss: 55499373084.6720\n",
      "Epoch 60/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 48498443266.7307 - val_loss: 54913080557.5680\n",
      "Epoch 61/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 47849067555.4987 - val_loss: 54233893208.0640\n",
      "Epoch 62/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 47215697621.4471 - val_loss: 53595765637.1200\n",
      "Epoch 63/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 46604395369.8133 - val_loss: 52944287367.1680\n",
      "Epoch 64/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 45990785888.7111 - val_loss: 52269192544.2560\n",
      "Epoch 65/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 45403824601.3156 - val_loss: 51679318474.7520\n",
      "Epoch 66/100\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44820922054.8836 - val_loss: 51057895211.0080\n",
      "Epoch 67/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 44251987851.4916 - val_loss: 50575721824.2560\n",
      "Epoch 68/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 43701218807.3529 - val_loss: 49871332540.4160\n",
      "Epoch 69/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 43184613272.2347 - val_loss: 49496116330.4960\n",
      "Epoch 70/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 42646651873.9627 - val_loss: 48767963824.1280\n",
      "Epoch 71/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 42152046355.3422 - val_loss: 48355462643.7120\n",
      "Epoch 72/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 41662766059.0649 - val_loss: 47823501885.4400\n",
      "Epoch 73/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 41186216316.4729 - val_loss: 47375007318.0160\n",
      "Epoch 74/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 40739703330.1333 - val_loss: 46878478696.4480\n",
      "Epoch 75/100\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 40317365680.3556 - val_loss: 46459855667.2000\n",
      "Epoch 76/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39904932105.7849 - val_loss: 46082868707.3280\n",
      "Epoch 77/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 39516416522.4676 - val_loss: 45725216833.5360\n",
      "Epoch 78/100\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 39136556702.8338 - val_loss: 45379310649.3440\n",
      "Epoch 79/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38787832334.1084 - val_loss: 44952361697.2800\n",
      "Epoch 80/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38449690081.5076 - val_loss: 44684161515.5200\n",
      "Epoch 81/100\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 38133527376.3271 - val_loss: 44239968174.0800\n",
      "Epoch 82/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37832159093.6462 - val_loss: 43944216133.6320\n",
      "Epoch 83/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37562114863.5591 - val_loss: 43706293223.4240\n",
      "Epoch 84/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37296628538.4818 - val_loss: 43477581234.1760\n",
      "Epoch 85/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37059986605.8524 - val_loss: 43332572971.0080\n",
      "Epoch 86/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36847266423.6942 - val_loss: 43139372875.7760\n",
      "Epoch 87/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 36641153799.5093 - val_loss: 42906087784.4480\n",
      "Epoch 88/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36469805598.4924 - val_loss: 42693839618.0480\n",
      "Epoch 89/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 36295136761.1733 - val_loss: 42580665204.7360\n",
      "Epoch 90/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36147872719.7582 - val_loss: 42457436717.0560\n",
      "Epoch 91/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 36002952351.2889 - val_loss: 42344374927.3600\n",
      "Epoch 92/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35881245416.5618 - val_loss: 42212974952.4480\n",
      "Epoch 93/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35759260853.1342 - val_loss: 42034412552.1920\n",
      "Epoch 94/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35646638802.7164 - val_loss: 41944127111.1680\n",
      "Epoch 95/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35531001950.6631 - val_loss: 41861161058.3040\n",
      "Epoch 96/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35426468036.6080 - val_loss: 41727931875.3280\n",
      "Epoch 97/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35333172157.5538 - val_loss: 41658260094.9760\n",
      "Epoch 98/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35240182389.8738 - val_loss: 41640363327.4880\n",
      "Epoch 99/100\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35166722173.6107 - val_loss: 41404345024.5120\n",
      "Epoch 100/100\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35087707595.6622 - val_loss: 41336059002.8800\n",
      "0.7244854712677341\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = nn_model()\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "            epochs=100, batch_size=128,\n",
    "            validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(model_score)\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, consider standardizing outputs as well\n",
    "This will get rid of all the noise caused by very narrow valleys - it'll transform it into a parabolic basin\n",
    "\n",
    "### Regularize:\n",
    "* Heavily parameterized models like neural networks are prone to overfitting\n",
    "* Popular off-the-shelf tools exist to regularize models and prevent overfitting: (basically prove that you're not doing it)\n",
    "    - L2 regularization (weight decay)\n",
    "    - Dropout\n",
    "    - Batch normalization\n",
    "    \n",
    "#### These tools come as standard Keras/TF layers!\n",
    "Add them randomly in between your layers to reduce overfitting.\n",
    "This should reduce the spread between your validation and training and testing errors.\n",
    "\n",
    "`model.add(keras.layers.Dropout(rate)`\n",
    "`model.add(keras.layers.ActivityRegularization(l1=0.0, l2=0.0)`\n",
    "`model.add(keras.layers.BatchNormalization())`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and model checkpointing:\n",
    "#### It's unlikely the last iteration is the best, and who knows how long until the thing is converged. Just grab the best validation error.\n",
    "\n",
    "Basically, we don't know how much it's bouncing around, and we don't know how long it'll take before it converges. So, we have the machine go on forever until it decides it's good enough to stop, and then we have it save the BEST model (not necessarily the last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/300\n",
      "18000/18000 [==============================] - 1s 37us/step - loss: 416651531110.1725 - val_loss: 456262317768.7040\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 456262317768.70398, saving model to best_model.h5\n",
      "Epoch 2/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 416208827070.6915 - val_loss: 455239381286.9120\n",
      "\n",
      "Epoch 00002: val_loss improved from 456262317768.70398 to 455239381286.91199, saving model to best_model.h5\n",
      "Epoch 3/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 414312701769.9555 - val_loss: 452138146463.7440\n",
      "\n",
      "Epoch 00003: val_loss improved from 455239381286.91199 to 452138146463.74402, saving model to best_model.h5\n",
      "Epoch 4/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 409837442374.7698 - val_loss: 445894139052.0320\n",
      "\n",
      "Epoch 00004: val_loss improved from 452138146463.74402 to 445894139052.03198, saving model to best_model.h5\n",
      "Epoch 5/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 402032916197.8311 - val_loss: 435999144411.1360\n",
      "\n",
      "Epoch 00005: val_loss improved from 445894139052.03198 to 435999144411.13599, saving model to best_model.h5\n",
      "Epoch 6/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 390324960712.0214 - val_loss: 421788836429.8240\n",
      "\n",
      "Epoch 00006: val_loss improved from 435999144411.13599 to 421788836429.82397, saving model to best_model.h5\n",
      "Epoch 7/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 374541301084.6151 - val_loss: 403465341501.4400\n",
      "\n",
      "Epoch 00007: val_loss improved from 421788836429.82397 to 403465341501.44000, saving model to best_model.h5\n",
      "Epoch 8/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 354834748393.2444 - val_loss: 381356076433.4080\n",
      "\n",
      "Epoch 00008: val_loss improved from 403465341501.44000 to 381356076433.40802, saving model to best_model.h5\n",
      "Epoch 9/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 331716756621.0845 - val_loss: 355613900275.7120\n",
      "\n",
      "Epoch 00009: val_loss improved from 381356076433.40802 to 355613900275.71198, saving model to best_model.h5\n",
      "Epoch 10/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 306029406133.3618 - val_loss: 327885322452.9920\n",
      "\n",
      "Epoch 00010: val_loss improved from 355613900275.71198 to 327885322452.99200, saving model to best_model.h5\n",
      "Epoch 11/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 278783971615.6302 - val_loss: 298668822888.4480\n",
      "\n",
      "Epoch 00011: val_loss improved from 327885322452.99200 to 298668822888.44800, saving model to best_model.h5\n",
      "Epoch 12/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 251240326336.0569 - val_loss: 269447887060.9920\n",
      "\n",
      "Epoch 00012: val_loss improved from 298668822888.44800 to 269447887060.99200, saving model to best_model.h5\n",
      "Epoch 13/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 224437629878.2720 - val_loss: 241465555681.2800\n",
      "\n",
      "Epoch 00013: val_loss improved from 269447887060.99200 to 241465555681.28000, saving model to best_model.h5\n",
      "Epoch 14/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 199382110947.1004 - val_loss: 215248404086.7840\n",
      "\n",
      "Epoch 00014: val_loss improved from 241465555681.28000 to 215248404086.78400, saving model to best_model.h5\n",
      "Epoch 15/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 176949616909.4258 - val_loss: 191907356082.1760\n",
      "\n",
      "Epoch 00015: val_loss improved from 215248404086.78400 to 191907356082.17599, saving model to best_model.h5\n",
      "Epoch 16/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 157579540765.8098 - val_loss: 171563220729.8560\n",
      "\n",
      "Epoch 00016: val_loss improved from 191907356082.17599 to 171563220729.85599, saving model to best_model.h5\n",
      "Epoch 17/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 141340181776.1564 - val_loss: 154387002687.4880\n",
      "\n",
      "Epoch 00017: val_loss improved from 171563220729.85599 to 154387002687.48801, saving model to best_model.h5\n",
      "Epoch 18/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 128154630109.4116 - val_loss: 140301129220.0960\n",
      "\n",
      "Epoch 00018: val_loss improved from 154387002687.48801 to 140301129220.09601, saving model to best_model.h5\n",
      "Epoch 19/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 117612578035.0293 - val_loss: 128907951013.8880\n",
      "\n",
      "Epoch 00019: val_loss improved from 140301129220.09601 to 128907951013.88800, saving model to best_model.h5\n",
      "Epoch 20/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 109237683311.9573 - val_loss: 119569585405.9520\n",
      "\n",
      "Epoch 00020: val_loss improved from 128907951013.88800 to 119569585405.95200, saving model to best_model.h5\n",
      "Epoch 21/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 102581123168.4836 - val_loss: 112178055086.0800\n",
      "\n",
      "Epoch 00021: val_loss improved from 119569585405.95200 to 112178055086.08000, saving model to best_model.h5\n",
      "Epoch 22/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 97162313596.9280 - val_loss: 106070665592.8320\n",
      "\n",
      "Epoch 00022: val_loss improved from 112178055086.08000 to 106070665592.83200, saving model to best_model.h5\n",
      "Epoch 23/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 92654650086.7413 - val_loss: 101045574762.4960\n",
      "\n",
      "Epoch 00023: val_loss improved from 106070665592.83200 to 101045574762.49600, saving model to best_model.h5\n",
      "Epoch 24/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 88839896971.4915 - val_loss: 96866870689.7920\n",
      "\n",
      "Epoch 00024: val_loss improved from 101045574762.49600 to 96866870689.79201, saving model to best_model.h5\n",
      "Epoch 25/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 85522171589.0631 - val_loss: 93261931216.8960\n",
      "\n",
      "Epoch 00025: val_loss improved from 96866870689.79201 to 93261931216.89600, saving model to best_model.h5\n",
      "Epoch 26/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 82621575856.1280 - val_loss: 90179273883.6480\n",
      "\n",
      "Epoch 00026: val_loss improved from 93261931216.89600 to 90179273883.64799, saving model to best_model.h5\n",
      "Epoch 27/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 80073073397.3049 - val_loss: 87459702177.7920\n",
      "\n",
      "Epoch 00027: val_loss improved from 90179273883.64799 to 87459702177.79201, saving model to best_model.h5\n",
      "Epoch 28/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 77839909493.8738 - val_loss: 85139722993.6640\n",
      "\n",
      "Epoch 00028: val_loss improved from 87459702177.79201 to 85139722993.66400, saving model to best_model.h5\n",
      "Epoch 29/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 75887386267.1929 - val_loss: 83128071749.6320\n",
      "\n",
      "Epoch 00029: val_loss improved from 85139722993.66400 to 83128071749.63200, saving model to best_model.h5\n",
      "Epoch 30/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 74163688374.2720 - val_loss: 81388617990.1440\n",
      "\n",
      "Epoch 00030: val_loss improved from 83128071749.63200 to 81388617990.14400, saving model to best_model.h5\n",
      "Epoch 31/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 72657513836.9991 - val_loss: 79828978827.2640\n",
      "\n",
      "Epoch 00031: val_loss improved from 81388617990.14400 to 79828978827.26401, saving model to best_model.h5\n",
      "Epoch 32/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 71328935976.0498 - val_loss: 78500433887.2320\n",
      "\n",
      "Epoch 00032: val_loss improved from 79828978827.26401 to 78500433887.23199, saving model to best_model.h5\n",
      "Epoch 33/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 70138950625.9627 - val_loss: 77246127013.8880\n",
      "\n",
      "Epoch 00033: val_loss improved from 78500433887.23199 to 77246127013.88800, saving model to best_model.h5\n",
      "Epoch 34/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 69074600321.9342 - val_loss: 76152987385.8560\n",
      "\n",
      "Epoch 00034: val_loss improved from 77246127013.88800 to 76152987385.85600, saving model to best_model.h5\n",
      "Epoch 35/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 68106764470.0444 - val_loss: 75202819063.8080\n",
      "\n",
      "Epoch 00035: val_loss improved from 76152987385.85600 to 75202819063.80800, saving model to best_model.h5\n",
      "Epoch 36/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 67202894178.9867 - val_loss: 74279482687.4880\n",
      "\n",
      "Epoch 00036: val_loss improved from 75202819063.80800 to 74279482687.48801, saving model to best_model.h5\n",
      "Epoch 37/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 66352770718.8338 - val_loss: 73330379849.7280\n",
      "\n",
      "Epoch 00037: val_loss improved from 74279482687.48801 to 73330379849.72800, saving model to best_model.h5\n",
      "Epoch 38/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 65524543579.0222 - val_loss: 72516421091.3280\n",
      "\n",
      "Epoch 00038: val_loss improved from 73330379849.72800 to 72516421091.32800, saving model to best_model.h5\n",
      "Epoch 39/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 64729789358.9902 - val_loss: 71648718880.7680\n",
      "\n",
      "Epoch 00039: val_loss improved from 72516421091.32800 to 71648718880.76801, saving model to best_model.h5\n",
      "Epoch 40/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63961853380.3804 - val_loss: 70903377559.5520\n",
      "\n",
      "Epoch 00040: val_loss improved from 71648718880.76801 to 70903377559.55200, saving model to best_model.h5\n",
      "Epoch 41/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 63193779310.1369 - val_loss: 70009842565.1200\n",
      "\n",
      "Epoch 00041: val_loss improved from 70903377559.55200 to 70009842565.12000, saving model to best_model.h5\n",
      "Epoch 42/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 62433857787.2213 - val_loss: 69298234851.3280\n",
      "\n",
      "Epoch 00042: val_loss improved from 70009842565.12000 to 69298234851.32800, saving model to best_model.h5\n",
      "Epoch 43/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 61684148695.4951 - val_loss: 68493229424.6400\n",
      "\n",
      "Epoch 00043: val_loss improved from 69298234851.32800 to 68493229424.64000, saving model to best_model.h5\n",
      "Epoch 44/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 60941060450.9867 - val_loss: 67640635457.5360\n",
      "\n",
      "Epoch 00044: val_loss improved from 68493229424.64000 to 67640635457.53600, saving model to best_model.h5\n",
      "Epoch 45/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 60198670652.7573 - val_loss: 66886025904.1280\n",
      "\n",
      "Epoch 00045: val_loss improved from 67640635457.53600 to 66886025904.12800, saving model to best_model.h5\n",
      "Epoch 46/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 59457777110.5849 - val_loss: 66185450553.3440\n",
      "\n",
      "Epoch 00046: val_loss improved from 66886025904.12800 to 66185450553.34400, saving model to best_model.h5\n",
      "Epoch 47/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 58723296888.6044 - val_loss: 65416069906.4320\n",
      "\n",
      "Epoch 00047: val_loss improved from 66185450553.34400 to 65416069906.43200, saving model to best_model.h5\n",
      "Epoch 48/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 57985131926.8693 - val_loss: 64674564866.0480\n",
      "\n",
      "Epoch 00048: val_loss improved from 65416069906.43200 to 64674564866.04800, saving model to best_model.h5\n",
      "Epoch 49/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 57253324536.0356 - val_loss: 63859461816.3200\n",
      "\n",
      "Epoch 00049: val_loss improved from 64674564866.04800 to 63859461816.32000, saving model to best_model.h5\n",
      "Epoch 50/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 56529127101.7813 - val_loss: 63085275283.4560\n",
      "\n",
      "Epoch 00050: val_loss improved from 63859461816.32000 to 63085275283.45600, saving model to best_model.h5\n",
      "Epoch 51/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 55790150993.6924 - val_loss: 62268503031.8080\n",
      "\n",
      "Epoch 00051: val_loss improved from 63085275283.45600 to 62268503031.80800, saving model to best_model.h5\n",
      "Epoch 52/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 55072407632.0996 - val_loss: 61423521562.6240\n",
      "\n",
      "Epoch 00052: val_loss improved from 62268503031.80800 to 61423521562.62400, saving model to best_model.h5\n",
      "Epoch 53/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 54341241729.4791 - val_loss: 60662887153.6640\n",
      "\n",
      "Epoch 00053: val_loss improved from 61423521562.62400 to 60662887153.66400, saving model to best_model.h5\n",
      "Epoch 54/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 53626606627.4987 - val_loss: 59973165678.5920\n",
      "\n",
      "Epoch 00054: val_loss improved from 60662887153.66400 to 59973165678.59200, saving model to best_model.h5\n",
      "Epoch 55/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52909167210.9511 - val_loss: 59282434260.9920\n",
      "\n",
      "Epoch 00055: val_loss improved from 59973165678.59200 to 59282434260.99200, saving model to best_model.h5\n",
      "Epoch 56/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 52206848566.1582 - val_loss: 58610305531.9040\n",
      "\n",
      "Epoch 00056: val_loss improved from 59282434260.99200 to 58610305531.90400, saving model to best_model.h5\n",
      "Epoch 57/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 51505851203.5840 - val_loss: 57760749125.6320\n",
      "\n",
      "Epoch 00057: val_loss improved from 58610305531.90400 to 57760749125.63200, saving model to best_model.h5\n",
      "Epoch 58/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 50811559360.7396 - val_loss: 57103361769.4720\n",
      "\n",
      "Epoch 00058: val_loss improved from 57760749125.63200 to 57103361769.47200, saving model to best_model.h5\n",
      "Epoch 59/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 50142957214.8338 - val_loss: 56334918844.4160\n",
      "\n",
      "Epoch 00059: val_loss improved from 57103361769.47200 to 56334918844.41600, saving model to best_model.h5\n",
      "Epoch 60/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 49461048373.7031 - val_loss: 55721538715.6480\n",
      "\n",
      "Epoch 00060: val_loss improved from 56334918844.41600 to 55721538715.64800, saving model to best_model.h5\n",
      "Epoch 61/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 48792859803.6480 - val_loss: 55022961754.1120\n",
      "\n",
      "Epoch 00061: val_loss improved from 55721538715.64800 to 55022961754.11200, saving model to best_model.h5\n",
      "Epoch 62/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 48147781353.4720 - val_loss: 54398813175.8080\n",
      "\n",
      "Epoch 00062: val_loss improved from 55022961754.11200 to 54398813175.80800, saving model to best_model.h5\n",
      "Epoch 63/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 47508183369.5004 - val_loss: 53725775527.9360\n",
      "\n",
      "Epoch 00063: val_loss improved from 54398813175.80800 to 53725775527.93600, saving model to best_model.h5\n",
      "Epoch 64/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 46892218724.8071 - val_loss: 53144240586.7520\n",
      "\n",
      "Epoch 00064: val_loss improved from 53725775527.93600 to 53144240586.75200, saving model to best_model.h5\n",
      "Epoch 65/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 46284976787.0009 - val_loss: 52591881289.7280\n",
      "\n",
      "Epoch 00065: val_loss improved from 53144240586.75200 to 52591881289.72800, saving model to best_model.h5\n",
      "Epoch 66/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 45694115665.2373 - val_loss: 51893800960.0000\n",
      "\n",
      "Epoch 00066: val_loss improved from 52591881289.72800 to 51893800960.00000, saving model to best_model.h5\n",
      "Epoch 67/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 45105797580.5724 - val_loss: 51236459872.2560\n",
      "\n",
      "Epoch 00067: val_loss improved from 51893800960.00000 to 51236459872.25600, saving model to best_model.h5\n",
      "Epoch 68/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 44551699402.2969 - val_loss: 50624457015.2960\n",
      "\n",
      "Epoch 00068: val_loss improved from 51236459872.25600 to 50624457015.29600, saving model to best_model.h5\n",
      "Epoch 69/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 44000806986.6382 - val_loss: 50059416469.5040\n",
      "\n",
      "Epoch 00069: val_loss improved from 50624457015.29600 to 50059416469.50400, saving model to best_model.h5\n",
      "Epoch 70/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 43472713737.1022 - val_loss: 49683858128.8960\n",
      "\n",
      "Epoch 00070: val_loss improved from 50059416469.50400 to 49683858128.89600, saving model to best_model.h5\n",
      "Epoch 71/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 42958255516.3307 - val_loss: 49126577471.4880\n",
      "\n",
      "Epoch 00071: val_loss improved from 49683858128.89600 to 49126577471.48800, saving model to best_model.h5\n",
      "Epoch 72/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 42464322904.9742 - val_loss: 48641481834.4960\n",
      "\n",
      "Epoch 00072: val_loss improved from 49126577471.48800 to 48641481834.49600, saving model to best_model.h5\n",
      "Epoch 73/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 41986802574.2222 - val_loss: 48101879119.8720\n",
      "\n",
      "Epoch 00073: val_loss improved from 48641481834.49600 to 48101879119.87200, saving model to best_model.h5\n",
      "Epoch 74/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 41537312427.5769 - val_loss: 47730488410.1120\n",
      "\n",
      "Epoch 00074: val_loss improved from 48101879119.87200 to 47730488410.11200, saving model to best_model.h5\n",
      "Epoch 75/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 41088850125.7102 - val_loss: 47298137227.2640\n",
      "\n",
      "Epoch 00075: val_loss improved from 47730488410.11200 to 47298137227.26400, saving model to best_model.h5\n",
      "Epoch 76/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 40674331936.5404 - val_loss: 46923307122.6880\n",
      "\n",
      "Epoch 00076: val_loss improved from 47298137227.26400 to 46923307122.68800, saving model to best_model.h5\n",
      "Epoch 77/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 40269565527.8364 - val_loss: 46410076323.8400\n",
      "\n",
      "Epoch 00077: val_loss improved from 46923307122.68800 to 46410076323.84000, saving model to best_model.h5\n",
      "Epoch 78/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39886304895.8862 - val_loss: 46089283928.0640\n",
      "\n",
      "Epoch 00078: val_loss improved from 46410076323.84000 to 46089283928.06400, saving model to best_model.h5\n",
      "Epoch 79/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 39515259422.4924 - val_loss: 45647776120.8320\n",
      "\n",
      "Epoch 00079: val_loss improved from 46089283928.06400 to 45647776120.83200, saving model to best_model.h5\n",
      "Epoch 80/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 39180846324.8498 - val_loss: 45377236828.1600\n",
      "\n",
      "Epoch 00080: val_loss improved from 45647776120.83200 to 45377236828.16000, saving model to best_model.h5\n",
      "Epoch 81/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 38843112226.8160 - val_loss: 45008169500.6720\n",
      "\n",
      "Epoch 00081: val_loss improved from 45377236828.16000 to 45008169500.67200, saving model to best_model.h5\n",
      "Epoch 82/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 38534711552.6827 - val_loss: 44749142425.6000\n",
      "\n",
      "Epoch 00082: val_loss improved from 45008169500.67200 to 44749142425.60000, saving model to best_model.h5\n",
      "Epoch 83/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 38231447306.2400 - val_loss: 44388054237.1840\n",
      "\n",
      "Epoch 00083: val_loss improved from 44749142425.60000 to 44388054237.18400, saving model to best_model.h5\n",
      "Epoch 84/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37957187330.9582 - val_loss: 44099545169.9200\n",
      "\n",
      "Epoch 00084: val_loss improved from 44388054237.18400 to 44099545169.92000, saving model to best_model.h5\n",
      "Epoch 85/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 37704399865.6284 - val_loss: 43831432773.6320\n",
      "\n",
      "Epoch 00085: val_loss improved from 44099545169.92000 to 43831432773.63200, saving model to best_model.h5\n",
      "Epoch 86/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 37467832862.4924 - val_loss: 43593162194.9440\n",
      "\n",
      "Epoch 00086: val_loss improved from 43831432773.63200 to 43593162194.94400, saving model to best_model.h5\n",
      "Epoch 87/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37224815285.5893 - val_loss: 43403617304.5760\n",
      "\n",
      "Epoch 00087: val_loss improved from 43593162194.94400 to 43403617304.57600, saving model to best_model.h5\n",
      "Epoch 88/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 37009303566.5636 - val_loss: 43225627361.2800\n",
      "\n",
      "Epoch 00088: val_loss improved from 43403617304.57600 to 43225627361.28000, saving model to best_model.h5\n",
      "Epoch 89/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 36809805736.6187 - val_loss: 42989770113.0240\n",
      "\n",
      "Epoch 00089: val_loss improved from 43225627361.28000 to 42989770113.02400, saving model to best_model.h5\n",
      "Epoch 90/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 36618986860.9991 - val_loss: 42738443190.2720\n",
      "\n",
      "Epoch 00090: val_loss improved from 42989770113.02400 to 42738443190.27200, saving model to best_model.h5\n",
      "Epoch 91/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 36443014720.1707 - val_loss: 42609259282.4320\n",
      "\n",
      "Epoch 00091: val_loss improved from 42738443190.27200 to 42609259282.43200, saving model to best_model.h5\n",
      "Epoch 92/300\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 36279575489.1947 - val_loss: 42535941472.2560\n",
      "\n",
      "Epoch 00092: val_loss improved from 42609259282.43200 to 42535941472.25600, saving model to best_model.h5\n",
      "Epoch 93/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 36140000884.9636 - val_loss: 42378356916.2240\n",
      "\n",
      "Epoch 00093: val_loss improved from 42535941472.25600 to 42378356916.22400, saving model to best_model.h5\n",
      "Epoch 94/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35991931289.6000 - val_loss: 42177376157.6960\n",
      "\n",
      "Epoch 00094: val_loss improved from 42378356916.22400 to 42177376157.69600, saving model to best_model.h5\n",
      "Epoch 95/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35856856616.5049 - val_loss: 42100882997.2480\n",
      "\n",
      "Epoch 00095: val_loss improved from 42177376157.69600 to 42100882997.24800, saving model to best_model.h5\n",
      "Epoch 96/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 35739578475.4062 - val_loss: 41944440373.2480\n",
      "\n",
      "Epoch 00096: val_loss improved from 42100882997.24800 to 41944440373.24800, saving model to best_model.h5\n",
      "Epoch 97/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35630458204.6151 - val_loss: 41865501376.5120\n",
      "\n",
      "Epoch 00097: val_loss improved from 41944440373.24800 to 41865501376.51200, saving model to best_model.h5\n",
      "Epoch 98/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35515240444.3591 - val_loss: 41759915147.2640\n",
      "\n",
      "Epoch 00098: val_loss improved from 41865501376.51200 to 41759915147.26400, saving model to best_model.h5\n",
      "Epoch 99/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35416981414.7982 - val_loss: 41624579932.1600\n",
      "\n",
      "Epoch 00099: val_loss improved from 41759915147.26400 to 41624579932.16000, saving model to best_model.h5\n",
      "Epoch 100/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35320500892.1031 - val_loss: 41564834562.0480\n",
      "\n",
      "Epoch 00100: val_loss improved from 41624579932.16000 to 41564834562.04800, saving model to best_model.h5\n",
      "Epoch 101/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35235966358.8693 - val_loss: 41453011664.8960\n",
      "\n",
      "Epoch 00101: val_loss improved from 41564834562.04800 to 41453011664.89600, saving model to best_model.h5\n",
      "Epoch 102/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35157008143.7013 - val_loss: 41362283790.3360\n",
      "\n",
      "Epoch 00102: val_loss improved from 41453011664.89600 to 41362283790.33600, saving model to best_model.h5\n",
      "Epoch 103/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 35076368454.9973 - val_loss: 41265333338.1120\n",
      "\n",
      "Epoch 00103: val_loss improved from 41362283790.33600 to 41265333338.11200, saving model to best_model.h5\n",
      "Epoch 104/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 35000985288.7040 - val_loss: 41247205163.0080\n",
      "\n",
      "Epoch 00104: val_loss improved from 41265333338.11200 to 41247205163.00800, saving model to best_model.h5\n",
      "Epoch 105/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34934010173.6676 - val_loss: 41235108921.3440\n",
      "\n",
      "Epoch 00105: val_loss improved from 41247205163.00800 to 41235108921.34400, saving model to best_model.h5\n",
      "Epoch 106/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34866903161.9698 - val_loss: 41154259779.5840\n",
      "\n",
      "Epoch 00106: val_loss improved from 41235108921.34400 to 41154259779.58400, saving model to best_model.h5\n",
      "Epoch 107/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34800923051.8044 - val_loss: 41115932164.0960\n",
      "\n",
      "Epoch 00107: val_loss improved from 41154259779.58400 to 41115932164.09600, saving model to best_model.h5\n",
      "Epoch 108/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34747926879.3458 - val_loss: 41053261922.3040\n",
      "\n",
      "Epoch 00108: val_loss improved from 41115932164.09600 to 41053261922.30400, saving model to best_model.h5\n",
      "Epoch 109/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34687302882.6453 - val_loss: 40907876302.8480\n",
      "\n",
      "Epoch 00109: val_loss improved from 41053261922.30400 to 40907876302.84800, saving model to best_model.h5\n",
      "Epoch 110/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34629310611.4560 - val_loss: 40906192977.9200\n",
      "\n",
      "Epoch 00110: val_loss improved from 40907876302.84800 to 40906192977.92000, saving model to best_model.h5\n",
      "Epoch 111/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34580229718.0160 - val_loss: 40820279246.8480\n",
      "\n",
      "Epoch 00111: val_loss improved from 40906192977.92000 to 40820279246.84800, saving model to best_model.h5\n",
      "Epoch 112/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34532480709.0631 - val_loss: 40768456949.7600\n",
      "\n",
      "Epoch 00112: val_loss improved from 40820279246.84800 to 40768456949.76000, saving model to best_model.h5\n",
      "Epoch 113/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34479538856.8462 - val_loss: 40724092420.0960\n",
      "\n",
      "Epoch 00113: val_loss improved from 40768456949.76000 to 40724092420.09600, saving model to best_model.h5\n",
      "Epoch 114/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34430870767.3885 - val_loss: 40612359045.1200\n",
      "\n",
      "Epoch 00114: val_loss improved from 40724092420.09600 to 40612359045.12000, saving model to best_model.h5\n",
      "Epoch 115/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34372497178.6240 - val_loss: 40664768249.8560\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 40612359045.12000\n",
      "Epoch 116/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34337554552.1493 - val_loss: 40603732344.8320\n",
      "\n",
      "Epoch 00116: val_loss improved from 40612359045.12000 to 40603732344.83200, saving model to best_model.h5\n",
      "Epoch 117/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34294715163.5342 - val_loss: 40415468716.0320\n",
      "\n",
      "Epoch 00117: val_loss improved from 40603732344.83200 to 40415468716.03200, saving model to best_model.h5\n",
      "Epoch 118/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34249113749.2764 - val_loss: 40401097752.5760\n",
      "\n",
      "Epoch 00118: val_loss improved from 40415468716.03200 to 40401097752.57600, saving model to best_model.h5\n",
      "Epoch 119/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34206875069.0987 - val_loss: 40362619240.4480\n",
      "\n",
      "Epoch 00119: val_loss improved from 40401097752.57600 to 40362619240.44800, saving model to best_model.h5\n",
      "Epoch 120/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 34160927841.3938 - val_loss: 40274878005.2480\n",
      "\n",
      "Epoch 00120: val_loss improved from 40362619240.44800 to 40274878005.24800, saving model to best_model.h5\n",
      "Epoch 121/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34125155112.2773 - val_loss: 40315128315.9040\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 40274878005.24800\n",
      "Epoch 122/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34084403227.3067 - val_loss: 40326759514.1120\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 40274878005.24800\n",
      "Epoch 123/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34052913124.6933 - val_loss: 40232124121.0880\n",
      "\n",
      "Epoch 00123: val_loss improved from 40274878005.24800 to 40232124121.08800, saving model to best_model.h5\n",
      "Epoch 124/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 34001200225.3938 - val_loss: 40265264791.5520\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 40232124121.08800\n",
      "Epoch 125/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33971141975.1538 - val_loss: 40132689723.3920\n",
      "\n",
      "Epoch 00125: val_loss improved from 40232124121.08800 to 40132689723.39200, saving model to best_model.h5\n",
      "Epoch 126/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33943605974.3573 - val_loss: 40115701874.6880\n",
      "\n",
      "Epoch 00126: val_loss improved from 40132689723.39200 to 40115701874.68800, saving model to best_model.h5\n",
      "Epoch 127/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33897856787.3422 - val_loss: 40120965038.0800\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 40115701874.68800\n",
      "Epoch 128/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33864024890.4818 - val_loss: 40042092068.8640\n",
      "\n",
      "Epoch 00128: val_loss improved from 40115701874.68800 to 40042092068.86400, saving model to best_model.h5\n",
      "Epoch 129/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33829092635.0791 - val_loss: 40084802109.4400\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 40042092068.86400\n",
      "Epoch 130/300\n",
      "18000/18000 [==============================] - 0s 10us/step - loss: 33803077841.3511 - val_loss: 40008068202.4960\n",
      "\n",
      "Epoch 00130: val_loss improved from 40042092068.86400 to 40008068202.49600, saving model to best_model.h5\n",
      "Epoch 131/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33757350892.8853 - val_loss: 39892724809.7280\n",
      "\n",
      "Epoch 00131: val_loss improved from 40008068202.49600 to 39892724809.72800, saving model to best_model.h5\n",
      "Epoch 132/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33732328255.9431 - val_loss: 39970004795.3920\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 39892724809.72800\n",
      "Epoch 133/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33701186761.1591 - val_loss: 39882179280.8960\n",
      "\n",
      "Epoch 00133: val_loss improved from 39892724809.72800 to 39882179280.89600, saving model to best_model.h5\n",
      "Epoch 134/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33662035217.9769 - val_loss: 39888902127.6160\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 39882179280.89600\n",
      "Epoch 135/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33630263077.5467 - val_loss: 39835953266.6880\n",
      "\n",
      "Epoch 00135: val_loss improved from 39882179280.89600 to 39835953266.68800, saving model to best_model.h5\n",
      "Epoch 136/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33599182372.8640 - val_loss: 39787887689.7280\n",
      "\n",
      "Epoch 00136: val_loss improved from 39835953266.68800 to 39787887689.72800, saving model to best_model.h5\n",
      "Epoch 137/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33573080159.5733 - val_loss: 39772491776.0000\n",
      "\n",
      "Epoch 00137: val_loss improved from 39787887689.72800 to 39772491776.00000, saving model to best_model.h5\n",
      "Epoch 138/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33538711215.2178 - val_loss: 39594914775.0400\n",
      "\n",
      "Epoch 00138: val_loss improved from 39772491776.00000 to 39594914775.04000, saving model to best_model.h5\n",
      "Epoch 139/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33510533773.5396 - val_loss: 39634985680.8960\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 39594914775.04000\n",
      "Epoch 140/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33481155370.0978 - val_loss: 39541736669.1840\n",
      "\n",
      "Epoch 00140: val_loss improved from 39594914775.04000 to 39541736669.18400, saving model to best_model.h5\n",
      "Epoch 141/300\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 33450035906.3324 - val_loss: 39623046397.9520\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 39541736669.18400\n",
      "Epoch 142/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33427562931.0862 - val_loss: 39604134772.7360\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 39541736669.18400\n",
      "Epoch 143/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33397577890.0196 - val_loss: 39551911198.7200\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 39541736669.18400\n",
      "Epoch 144/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33368586862.5920 - val_loss: 39563687591.9360\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 39541736669.18400\n",
      "Epoch 145/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33344884486.5991 - val_loss: 39538235801.6000\n",
      "\n",
      "Epoch 00145: val_loss improved from 39541736669.18400 to 39538235801.60000, saving model to best_model.h5\n",
      "Epoch 146/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33313006991.5876 - val_loss: 39474003968.0000\n",
      "\n",
      "Epoch 00146: val_loss improved from 39538235801.60000 to 39474003968.00000, saving model to best_model.h5\n",
      "Epoch 147/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33294530782.0942 - val_loss: 39507168264.1920\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 39474003968.00000\n",
      "Epoch 148/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33264469901.3120 - val_loss: 39398908952.5760\n",
      "\n",
      "Epoch 00148: val_loss improved from 39474003968.00000 to 39398908952.57600, saving model to best_model.h5\n",
      "Epoch 149/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33238611214.3360 - val_loss: 39367746289.6640\n",
      "\n",
      "Epoch 00149: val_loss improved from 39398908952.57600 to 39367746289.66400, saving model to best_model.h5\n",
      "Epoch 150/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33222407531.1787 - val_loss: 39297145339.9040\n",
      "\n",
      "Epoch 00150: val_loss improved from 39367746289.66400 to 39297145339.90400, saving model to best_model.h5\n",
      "Epoch 151/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33195723079.6800 - val_loss: 39247766749.1840\n",
      "\n",
      "Epoch 00151: val_loss improved from 39297145339.90400 to 39247766749.18400, saving model to best_model.h5\n",
      "Epoch 152/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33173987422.6631 - val_loss: 39204825530.3680\n",
      "\n",
      "Epoch 00152: val_loss improved from 39247766749.18400 to 39204825530.36800, saving model to best_model.h5\n",
      "Epoch 153/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33150514440.8747 - val_loss: 39288190992.3840\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 39204825530.36800\n",
      "Epoch 154/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33126561375.1182 - val_loss: 39210751000.5760\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 39204825530.36800\n",
      "Epoch 155/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33107829935.6729 - val_loss: 39114096246.7840\n",
      "\n",
      "Epoch 00155: val_loss improved from 39204825530.36800 to 39114096246.78400, saving model to best_model.h5\n",
      "Epoch 156/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33080987717.1769 - val_loss: 39164249440.2560\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 39114096246.78400\n",
      "Epoch 157/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33065026582.7556 - val_loss: 39083527667.7120\n",
      "\n",
      "Epoch 00157: val_loss improved from 39114096246.78400 to 39083527667.71200, saving model to best_model.h5\n",
      "Epoch 158/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 33050029606.6844 - val_loss: 39102963548.1600\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 39083527667.71200\n",
      "Epoch 159/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 33012575812.7218 - val_loss: 39161105121.2800\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 39083527667.71200\n",
      "Epoch 160/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32997535485.4969 - val_loss: 39153530667.0080\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 39083527667.71200\n",
      "Epoch 161/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32985147950.8764 - val_loss: 39080671248.3840\n",
      "\n",
      "Epoch 00161: val_loss improved from 39083527667.71200 to 39080671248.38400, saving model to best_model.h5\n",
      "Epoch 162/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32963254812.6720 - val_loss: 39125891776.5120\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 39080671248.38400\n",
      "Epoch 163/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32941367434.3538 - val_loss: 39180719718.4000\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 39080671248.38400\n",
      "Epoch 164/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32923472449.0809 - val_loss: 38963063062.5280\n",
      "\n",
      "Epoch 00164: val_loss improved from 39080671248.38400 to 38963063062.52800, saving model to best_model.h5\n",
      "Epoch 165/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32898860846.6489 - val_loss: 39026718081.0240\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 38963063062.52800\n",
      "Epoch 166/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32880595361.7920 - val_loss: 38959094759.4240\n",
      "\n",
      "Epoch 00166: val_loss improved from 38963063062.52800 to 38959094759.42400, saving model to best_model.h5\n",
      "Epoch 167/300\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32856920058.5387 - val_loss: 38985285730.3040\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 38959094759.42400\n",
      "Epoch 168/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32839054204.0178 - val_loss: 38950857801.7280\n",
      "\n",
      "Epoch 00168: val_loss improved from 38959094759.42400 to 38950857801.72800, saving model to best_model.h5\n",
      "Epoch 169/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32834247134.7769 - val_loss: 38986128064.5120\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 38950857801.72800\n",
      "Epoch 170/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32807815564.8569 - val_loss: 38879601033.2160\n",
      "\n",
      "Epoch 00170: val_loss improved from 38950857801.72800 to 38879601033.21600, saving model to best_model.h5\n",
      "Epoch 171/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32788788066.5316 - val_loss: 38821070897.1520\n",
      "\n",
      "Epoch 00171: val_loss improved from 38879601033.21600 to 38821070897.15200, saving model to best_model.h5\n",
      "Epoch 172/300\n",
      "18000/18000 [==============================] - 0s 13us/step - loss: 32769780587.6338 - val_loss: 38896522592.2560\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 38821070897.15200\n",
      "Epoch 173/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32758137607.5093 - val_loss: 38761004138.4960\n",
      "\n",
      "Epoch 00173: val_loss improved from 38821070897.15200 to 38761004138.49600, saving model to best_model.h5\n",
      "Epoch 174/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32730617559.2676 - val_loss: 38817983430.6560\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 38761004138.49600\n",
      "Epoch 175/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32720262385.2089 - val_loss: 38828494848.0000\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 38761004138.49600\n",
      "Epoch 176/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32704084120.9173 - val_loss: 38765275840.5120\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 38761004138.49600\n",
      "Epoch 177/300\n",
      "18000/18000 [==============================] - 0s 11us/step - loss: 32687369322.4960 - val_loss: 38839746363.3920\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 38761004138.49600\n",
      "Epoch 178/300\n",
      "18000/18000 [==============================] - 0s 12us/step - loss: 32665750028.2880 - val_loss: 38768086941.6960\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 38761004138.49600\n",
      "Model score: 0.6819109646034195\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set callback functions to early stop training and save the \n",
    "# best model so far\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5), #patience = how many iterations without improvement before it just stops\n",
    "             ModelCheckpoint(filepath='best_model.h5',\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)]\n",
    "\n",
    "model = nn_model(nodes=50)\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = model.fit(selected_feature_train, price_train,\n",
    "                   epochs=300, callbacks=callbacks, batch_size=128,\n",
    "                   validation_data=(selected_feature_val, price_val))\n",
    "model_score = score(model.predict(selected_feature_val), price_val)\n",
    "print(f\"Model score: {model_score}\")\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You don't have to remember these resources because they're here when you need them\n",
    "https://www.tensorflow.org/api_docs\n",
    "\n",
    "https://keras.io/\n",
    "\n",
    "https://www.tensorflow.org/tutorials/\n",
    "\n",
    "https://www.google.com\n",
    "\n",
    "### Don't trust me, trust your validation errors\n",
    "### Don't look at your test set until you're actually going to test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
