{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification -- Images & Hands-On\n",
    "\n",
    "## Table of Contents\n",
    "<ol>\n",
    "    <li>Processing of complicated data like images</li>\n",
    "    <li>Thinking about models to use for image classification</li>\n",
    "    <li>Implementation of common models</li>\n",
    "    <li>Convolutional neural networks -- an ML greatest hit</li>\n",
    "</ol>\n",
    "\n",
    "## 1. Processing of complicated data like images\n",
    "\n",
    "#### Suppose we begin with colored 32 x 32 pixel images of objects we wish to classify.\n",
    "\n",
    "![](cifar.png)\n",
    "<span style=\"font-size:0.75em;\">CIFAR-10 Krizhevsky et al.</span>\n",
    "\n",
    "CIFAR-10 is a pretty standard, low-res data set for testing image classification. \n",
    "\n",
    "### How can we encode the information from one image?\n",
    "![](corgis.png)\n",
    "![](doge.png)\n",
    "<span style=\"font-size:0.75em;\">commonlounge.com; subsubroutine.com</span>\n",
    "\n",
    "Each pixel has three dimensions, which means that its dimension is 32 x 32 x 3.\n",
    "\n",
    "### Let's start with a simpler example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n",
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do MNIST instead of CIPHAR-10 for simplicity's and time's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "(X, y), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know more about the data that we just loaded, so that we know what all we can do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(60000, 28, 28) (60000,)\n",
      "(10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(X.shape, y.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we verify that we have images, and specifically the type of image we expect to have - just show one of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2645630f2b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we make sure our data makes sense - we know we have colors, so the max shoud be 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "print(np.max(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As long as each data point is of the same shape, we can unroll these 2- or 3-tensors into long vectors\n",
    "- How many dimensions in each CIFAR-10 data point? Remember this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "#Flatten each X\n",
    "X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
    "# instead of being in a grid, all the images are now 1-D vectors. \n",
    "#We leave the first dimension alone because it functions as the list of samples. \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our classification output will be a vector of 0s except for the target class, which should be a 1.\n",
    "### Presently our output is instead encoded as a single ordinal variable between 0 and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "y_ord = y # In case we need this later\n",
    "\n",
    "# Encode each element of y as 10-length \"one-hot vector\" with binary elements\n",
    "#basically, we're making an e-basis vector for each number\n",
    "y = keras.utils.to_categorical(y)\n",
    "print(y.shape)\n",
    "print(y[0])\n",
    "#So, now the y-value of our data is an indicative vector instead of a digit between 1 and 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lastly, let's choose an error metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(true, pred):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if np.sum(np.dot(true[i], pred[i])) == 1.0:\n",
    "            acc += 1\n",
    "    score = acc / len(true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Thinking about models to use for image classification\n",
    "\n",
    "### k-Nearest neighbors\n",
    "\n",
    "* 1-Nearest Neighbors (aka nearest neighbors)\n",
    "    - Use some distance metric to compare each 784-D vector to all training examples\n",
    "    - Order samples by distance\n",
    "    - Classify the same as the smallest distance example\n",
    "* k-Nearest Neighbors\n",
    "    - Classify by committee based on several small distances\n",
    "    - Get a better sense of which cluster you're in\n",
    "* Where do these fail?\n",
    "    - When you don't get a consensus from the committee\n",
    "* How do these scale with training examples?\n",
    "    - scaling of n - you have to look at every single data point for every single inference you want to make - not very fast\n",
    "\n",
    "### Logistic regression\n",
    "* Optimal parameters attained from maximizing the likelihood of dataset, aka minimizing the negative log-likelihood\n",
    "$$\\mathcal{L}(\\theta = \\{W,b\\},\\mathcal{D}) = \\sum_{i=0}^{|\\mathcal{D}|} log(P(Y = y^{(i)} | x^{(i)},W,b))$$\n",
    "![](log_reg.png)\n",
    "    - basically, we're minimizing loss\n",
    "    - After we find a function, we have to look at the errors we get, analyze the situation, and back-propogate the error and make changes\n",
    "    - the probabilities are always good to keep around to see where problem data is or areas where we need more data\n",
    "* \"nonlinear\" -- though always depends directly on weighted sums of pixels\n",
    "\n",
    "### Random forest classifiers\n",
    "* \"Split\" predictions based on pixels or collection of pixels\n",
    "* Truly nonlinear\n",
    "* Basically deductive reasoning\n",
    "\n",
    "### Feed-forward neural networks\n",
    "* Nonlinear\n",
    "* Permits \"communication\" between pixels via dense layers\n",
    "* Not super impressive, to be honest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation of common models\n",
    "### WAIT what haven't I done yet?\n",
    "\n",
    "VALIDATION!!! Split out your validation set from your train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train-val set beore model choosing\n",
    "#Cross-validation would be nice, but we don't really have time to do that\n",
    "\n",
    "X_train = X[:54000]\n",
    "y_train = y[:54000]\n",
    "y_ord_train = y_ord[:54000] #We're keeping this around because some of the models we're about to use prefer it\n",
    "\n",
    "X_val = X[54000:]\n",
    "y_val = y[54000:]\n",
    "y_ord_val = y_ord[54000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val[:100])\n",
    "print(score(y_val[:100], preds)) #Chose only the first 100 because it'll take a really long time otherwise\n",
    "#97% accuracy is pretty good! But it's still reeaaaalllllyyyyyy slow\n",
    "#This isn't really training, just comparing distances - basically memorizing a bunch of vectors, which is not the same thing as ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 43 epochs took 24 seconds\n",
      "convergence after 41 epochs took 24 seconds\n",
      "convergence after 41 epochs took 23 seconds\n",
      "convergence after 53 epochs took 30 seconds\n",
      "convergence after 45 epochs took 28 seconds\n",
      "convergence after 48 epochs took 27 seconds\n",
      "convergence after 53 epochs took 30 seconds\n",
      "convergence after 47 epochs took 27 seconds\n",
      "convergence after 53 epochs took 31 seconds\n",
      "convergence after 47 epochs took 27 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  4.5min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(solver='saga', tol=0.01, verbose=1) \n",
    "#All these parameters - don't expect to get it right (so it converges) the first time\n",
    "\n",
    "model.fit(X_train, y_ord_train) #this one likes y_ord\n",
    "preds = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create equivalent accuracy score for ordinal outputs\n",
    "def score_ord(true, preds):\n",
    "    acc = 0\n",
    "    for i in range(len(true)):\n",
    "        if true[i] == preds[i]:\n",
    "            acc += 1\n",
    "    score = acc / len(true)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.931\n"
     ]
    }
   ],
   "source": [
    "print(score_ord(y_ord_val,preds))\n",
    "#93% accuracy\n",
    "#vvvvvvvveeeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrryyyyyyyyyyyyyyyyyyyyyyyyyyyyy slow - sun at own risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifiers - you try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8785\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier() #normally, you'd want to \n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "print(score(y_val, preds))\n",
    "#87% accuracy - not great\n",
    "#That's why you mess with parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9708333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#The Classifier has a little more stuff than just the regressor\n",
    "#Remember that sklearn is mostly good for quickly choosing your model\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,50,20))\n",
    "model.fit(X_train, y_train)\n",
    "preds = model.predict(X_val)\n",
    "print(score(y_val, preds))\n",
    "#97% accuracy - up there with k-nearest neighbors\n",
    "#Inference time is a lot better for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](conv_net.png)\n",
    "<span style=\"font-size:0.75em;\">easy-tensorflow.com</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a more intelligent system than just densely propogating everything. Convolutional neural networks take how our brains work more into account. Basically, we're looking for individual features - lines, curves, edges, etc. The filter gets dragged across the image and looks for what it wants, multiplying pixels elementwise and doing other operations to get a number that represents the value of what the filter is looking for in that image. That's called a convolution. We do a ton of those with a ton of different filters to create a bunch of different feature maps. Then we repeat, and each successive feature map gets more and more macroscopic, eventually recognizing the entire image (hopefully). The pooling layers reduce the dimensionality so that we don't get overwhelmed. At the end, hopefully we have something with all of the information about the image. When we think we have enough feature maps, we flatten it out and do a dense connection just like we were doing before. If we have conditioned the huge number of weights well enough, it will be accurate. \n",
    "\n",
    "Basically, it's looking at areas of the image, instead of each pixel individually. \n",
    "\n",
    "https://towardsdatascience.com/convolutional-neural-network-17fb77e76c05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Generally, a <i>filter</i> is a rectangular $k \\times l$ matrix of weights.\n",
    "* A single <i>filter</i> traverses an image, elementwise multiplying pixels in its range, adding, and performing a nonlinearity.\n",
    "* The new <i>feature map</i> generated is typically the same size or smaller than the input.\n",
    "* Many <i>feature maps</i> are generated with different <i>filters</i>, each with different weights\n",
    "* <i>Pooling layers</i> serve to reduce feature map dimensionality.\n",
    "* The CNN concludes with generic Dense layers\n",
    "\n",
    "### Examines local areas of photographs -- takes full photo matrix as input, not flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You don't even have to flatten it out! \n",
    "(X, y), (X_test, y_test) = mnist.load_data()\n",
    "y_train = keras.utils.to_categorical(y)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose I have validated the following hyperparameters such that I believe they are optimal. <i>Now</i> we can test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose train and test, add an additional 1 rank to indicate\n",
    "# greyscale for special layers\n",
    "\n",
    "X_train = X.reshape(60000, 28, 28, 1)\n",
    "X_test = X_test.reshape(10000, 28, 28, 1)\n",
    "#Using test because we're going to pretend that we've already built and validated it and we're ready to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/Keras-2.2.4-py3.7.egg/keras/backend/tensorflow_backend.py:3651: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/25\n",
      "60000/60000 [==============================] - 227s 4ms/step - loss: 0.2564 - acc: 0.9220\n",
      "Epoch 2/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0658 - acc: 0.9803\n",
      "Epoch 3/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0503 - acc: 0.9848\n",
      "Epoch 4/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0414 - acc: 0.9872\n",
      "Epoch 5/25\n",
      "60000/60000 [==============================] - 230s 4ms/step - loss: 0.0352 - acc: 0.9890\n",
      "Epoch 6/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0328 - acc: 0.9900\n",
      "Epoch 7/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0276 - acc: 0.9916\n",
      "Epoch 8/25\n",
      "60000/60000 [==============================] - 232s 4ms/step - loss: 0.0259 - acc: 0.9918\n",
      "Epoch 9/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0240 - acc: 0.9927\n",
      "Epoch 10/25\n",
      "60000/60000 [==============================] - 235s 4ms/step - loss: 0.0232 - acc: 0.9928\n",
      "Epoch 11/25\n",
      "60000/60000 [==============================] - 245s 4ms/step - loss: 0.0216 - acc: 0.9933\n",
      "Epoch 12/25\n",
      "60000/60000 [==============================] - 243s 4ms/step - loss: 0.0190 - acc: 0.9940\n",
      "Epoch 13/25\n",
      "60000/60000 [==============================] - 254s 4ms/step - loss: 0.0220 - acc: 0.9929\n",
      "Epoch 14/25\n",
      "60000/60000 [==============================] - 264s 4ms/step - loss: 0.0189 - acc: 0.9943\n",
      "Epoch 15/25\n",
      "60000/60000 [==============================] - 274s 5ms/step - loss: 0.0155 - acc: 0.9950\n",
      "Epoch 16/25\n",
      "60000/60000 [==============================] - 296s 5ms/step - loss: 0.0165 - acc: 0.9948\n",
      "Epoch 17/25\n",
      "60000/60000 [==============================] - 309s 5ms/step - loss: 0.0151 - acc: 0.9956\n",
      "Epoch 18/25\n",
      "60000/60000 [==============================] - 301s 5ms/step - loss: 0.0124 - acc: 0.9959\n",
      "Epoch 19/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0137 - acc: 0.9957\n",
      "Epoch 20/25\n",
      "60000/60000 [==============================] - 235s 4ms/step - loss: 0.0116 - acc: 0.9963\n",
      "Epoch 21/25\n",
      "60000/60000 [==============================] - 233s 4ms/step - loss: 0.0126 - acc: 0.9957\n",
      "Epoch 22/25\n",
      "60000/60000 [==============================] - 234s 4ms/step - loss: 0.0103 - acc: 0.9966\n",
      "Epoch 23/25\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.0103 - acc: 0.9966\n",
      "Epoch 24/25\n",
      "60000/60000 [==============================] - 237s 4ms/step - loss: 0.0097 - acc: 0.9968\n",
      "Epoch 25/25\n",
      "60000/60000 [==============================] - 231s 4ms/step - loss: 0.0108 - acc: 0.9966\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Dropout, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=3,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "#Coresponds to the flat grey blocks in the picture above\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=25, verbose=1)\n",
    "\n",
    "#Not going to enter and you probably shouldn't either - this is a recreation of state of the art framework, and it takes about 2 hours to run\n",
    "# When you consider the number of hyperparameters involoved, you can see how optimizing this would be an actual nightmare\n",
    "# Though, in the world of data science, 2 hours is really not that bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](KaggleMNIST.png)\n",
    "<span style=\"font-size:0.75em;\">Kaggle - Chris Deotte 2018</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9953\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)\n",
    "final = np.zeros_like(preds)\n",
    "final[np.arange(len(preds)), preds.argmax(1)] = 1\n",
    "\n",
    "print(score(y_test, final))\n",
    "#99.53% output - that's pretty good - 1 in 200 is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"nice_cnn.h5\")\n",
    "#The convolutional neural networks become more important with more intense data sets\n",
    "# It's very hard, if not impossible, to get good accuracy with KNN on something like CIPHAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word on inductive bias and domain knowledge\n",
    "* CNNs take advantage of our understanding of local features in mapping images to semantic meaning (number labels, dog/cat/plane)\n",
    "* \"A universal function approximator\": The infinitely large dense NN can fit any analytic function exactly with enough data.\n",
    "    - \"Not really\": We rarely have \"enough data\" and can't train infinitely large NNs\n",
    "    - The name of the game is making the network size and data requirements <i>practical</i>\n",
    "* The state of the art usually comes from understanding your problem first"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
